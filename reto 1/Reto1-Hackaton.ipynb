{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "42avAvSg17by",
      "metadata": {
        "id": "42avAvSg17by"
      },
      "outputs": [],
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "\n",
        "\n",
        "XLA_USE_BF16=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "318d1165-92db-4612-a238-bb143178bf2a",
      "metadata": {
        "id": "318d1165-92db-4612-a238-bb143178bf2a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image as Img\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "from torchvision.datasets.utils import download_url\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch.backends\n",
        "import zipfile\n",
        "torch.backends.cudnn.enabled = False\n",
        "#CUDA_LAUNCH_BLOCKING=1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wre_3M3H6hQR",
      "metadata": {
        "id": "wre_3M3H6hQR"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Hy-8tyhp6i8g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy-8tyhp6i8g",
        "outputId": "a7156ec5-eebe-41f8-e669-07e0202fa517"
      },
      "outputs": [],
      "source": [
        "dataset_url = r\"https://challenges-asset-files.s3.us-east-2.amazonaws.com/Events/Oracle+Liga+Uni/1r+Reto/1st_oracle_challenge.zip\"\n",
        "\n",
        "# Descargar dataset si no existe\n",
        "if not os.path.exists(r\"./dataset\"):\n",
        "    print(\">>>Descargando Datos\")\n",
        "    download_url(dataset_url, r\"./dataset\", \"data.zip\")\n",
        "    with zipfile.ZipFile(r\"./dataset/data.zip\") as zip_dataset:\n",
        "        print(\">>>Descomprimiendo Dataset\")\n",
        "        zip_dataset.extractall(r\"./dataset\")\n",
        "    os.remove(r\"./dataset/data.zip\")\n",
        "    os.rename(\"./dataset/imgs\",\"./dataset/all_imgs\")\n",
        "\n",
        "dataset_folder_path: str = r\"./dataset\"\n",
        "train_set_path: str = r\"./dataset/train.csv\"\n",
        "test_set_path: str = r\"./dataset/test_no_label.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ebb83022-e2f5-41c9-aebf-e06a4593585f",
      "metadata": {
        "id": "ebb83022-e2f5-41c9-aebf-e06a4593585f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "archivo_train = r\"./dataset/train.csv\"\n",
        "archivo_test = r\"./dataset/test_no_label.csv\"\n",
        "dir_imagenes = r\"./dataset/all_imgs/\"\n",
        "\n",
        "torch.cuda.max_memory_allocated(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1f4d648b-32e8-446b-b3b9-132d927f4d45",
      "metadata": {
        "id": "1f4d648b-32e8-446b-b3b9-132d927f4d45"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset : pd.DataFrame = pd.read_csv(archivo_train, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bEo0tcDJ9va8",
      "metadata": {
        "id": "bEo0tcDJ9va8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    path_img  label\n",
            "idx_train                                                          \n",
            "0          all_imgs/bc7696f4-1460-4d0b-a63d-f84b3be4da0f....      0\n",
            "1          all_imgs/f8d50663-60d8-4da5-a8b8-79f954aec503....      2\n",
            "2          all_imgs/51df0f29-758b-4741-ab74-a0ff8e21c044....      4\n",
            "3          all_imgs/f61b81d3-3b79-4162-b4d6-4f1b39518c4c....      0\n",
            "4          all_imgs/b21e0668-bd09-4794-9e90-da8ecffc4c1c....      0\n"
          ]
        }
      ],
      "source": [
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2f4b04c7-e386-4f22-8423-7a6178e9d96f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4b04c7-e386-4f22-8423-7a6178e9d96f",
        "outputId": "c45d57e1-ccaa-4b02-db94-bc6299016bf7"
      },
      "outputs": [],
      "source": [
        "# Estratificar los dataset para que comprenda todas las clases por igual.\n",
        "train_dataset, val_dataset = train_test_split(\n",
        "    dataset, test_size=0.3, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b111cd7-387f-420f-bc08-34ad9db1b813",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "3b111cd7-387f-420f-bc08-34ad9db1b813",
        "outputId": "8834f494-0c18-4654-cfa3-0cd0a8618f5f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path_img</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx_train</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>all_imgs/bc7696f4-1460-4d0b-a63d-f84b3be4da0f....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    path_img  label\n",
              "idx_train                                                          \n",
              "0          all_imgs/bc7696f4-1460-4d0b-a63d-f84b3be4da0f....      0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7956c142-b6ea-458e-8bb7-8e3f16b73381",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "7956c142-b6ea-458e-8bb7-8e3f16b73381",
        "outputId": "16914fe7-babb-4bd4-b1a1-5f3c1d8899f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x1cba05e8be0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHpCAYAAACmzsSXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtsElEQVR4nO3df1TUdb7H8dfETzWcBIORGyYVmYaZi2VSu9KqkDeijvfGdi2yzcquP0nth9dtpfYGZaVsWJZer5hkdO656do9mwr9oFy0FKOENasT5Y+FyBYHMBoIvvePPX5PI2DKr/kgz8c5c47z/X5m5v1Vjk++zAzjsCzLEgAAMM45vh4AAAC0jUgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGI9GmyLEu1tbXibeUAgJ5CpE9TXV2dnE6n6urqfD0KAKCPINIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCh/Xw+A3uPgwYM6evRot9z34MGDNXTo0G65bwDorYg0TsvBgwd12WUj1NDwfbfcf79+/fXpp/sJNQD8BJHGaTl69KgaGr7XuLuXauCQYV1637WVX+mD/35MR48eJdIA8BNEGmdk4JBhCh063NdjAECfwAvHAAAwFJEGAMBQRBoAAEP59Dnp9957T08//bRKSkpUWVmpTZs26ZZbbmlz7cyZM7V69WqtWLFC6enp9naPx6NFixbp1VdfVUNDgyZOnKgXXnhBF1xwgb2mpqZG8+bN05YtWyRJKSkpysnJ0XnnndeNRwcAvR9vvfQtn0b6+PHjGj16tH7729/qX/7lX9pdt3nzZn3wwQeKjIxstS89PV1vvPGG8vPzFRYWpoULFyo5OVklJSXy8/OTJE2bNk2HDx/W1q1bJUn33Xef0tLS9MYbb3TPgQHAT/TW0PXmt1721r/zk/k00lOmTNGUKVNOuebIkSOaM2eOtm3bphtvvNFrn9vt1tq1a7VhwwZNmjRJkpSXl6eoqCgVFhYqKSlJ+/fv19atW7Vr1y6NGzdOkrRmzRqNHz9eBw4c0PDhbb9S2ePxyOPx2Ndra2s7c6gA+qjeHLre+tbL3vx3fjKj34LV0tKitLQ0Pfjgg7r88stb7S8pKVFTU5MSExPtbZGRkYqNjVVxcbGSkpK0c+dOOZ1OO9CSdM0118jpdKq4uLjdSGdlZemxxx7r+oPS2fMdHroXXydnh94aup/qbW+9PBv+zk8wOtJPPfWU/P39NW/evDb3V1VVKTAwUIMGDfLaHhERoaqqKntNeHh4q9uGh4fba9qyePFiLViwwL5eW1urqKiojhyGl7PpOzx0H75Ozj69LXRng7Ph79zYSJeUlOiPf/yj9u7dK4fDcUa3tSzL6zZt3f7kNScLCgpSUFDQGT3u6TibvsND9+HrBIBkcKTff/99VVdXe/0n0tzcrIULFyo7O1tfffWVXC6XGhsbVVNT43U2XV1drfj4eEmSy+XSN9980+r+v/32W0VERHT/gbTjbPgOD92PrxOgbzM20mlpafaLwU5ISkpSWlqafvvb30qS4uLiFBAQoIKCAqWmpkqSKisrVVZWpmXLlkmSxo8fL7fbrQ8//FBXX321JOmDDz6Q2+22Qw6gd+B5evQ1Po10fX29vvjiC/t6RUWFSktLFRoaqqFDhyosLMxrfUBAgFwul/1iL6fTqRkzZmjhwoUKCwtTaGioFi1apFGjRtmBHzFihG644Qbde++9eumllyT94y1YycnJ7b5oDGcX/mM/O/A8Pfoin0Z6z549uv766+3rJ16oNX36dOXm5p7WfaxYsUL+/v5KTU21f5lJbm6u/R5pSXrllVc0b948+1XgKSkpWrlyZdcdCIzFf+xnD56nR1/k00gnJCTIsqzTXv/VV1+12hYcHKycnBzl5OS0e7vQ0FDl5eV1ZET0cvzHfvbheXr0JcY+Jw10Jf5jB9Ab8QEbAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhiLSAAAYikgDAGAoIg0AgKGINAAAhvJppN977z3ddNNNioyMlMPh0ObNm+19TU1NevjhhzVq1CgNGDBAkZGRuvPOO/W3v/3N6z48Ho/mzp2rwYMHa8CAAUpJSdHhw4e91tTU1CgtLU1Op1NOp1NpaWk6duxYDxwhAAAd59NIHz9+XKNHj9bKlStb7fv++++1d+9ePfroo9q7d69ef/11ffbZZ0pJSfFal56erk2bNik/P187duxQfX29kpOT1dzcbK+ZNm2aSktLtXXrVm3dulWlpaVKS0vr9uMDAKAz/H354FOmTNGUKVPa3Od0OlVQUOC1LScnR1dffbUOHjyooUOHyu12a+3atdqwYYMmTZokScrLy1NUVJQKCwuVlJSk/fv3a+vWrdq1a5fGjRsnSVqzZo3Gjx+vAwcOaPjw4d17kAAAdFCvek7a7XbL4XDovPPOkySVlJSoqalJiYmJ9prIyEjFxsaquLhYkrRz5045nU470JJ0zTXXyOl02mva4vF4VFtb63UBAKAn9ZpI//DDD3rkkUc0bdo0DRw4UJJUVVWlwMBADRo0yGttRESEqqqq7DXh4eGt7i88PNxe05asrCz7OWyn06moqKguPBoAAH5er4h0U1OTbrvtNrW0tOiFF1742fWWZcnhcNjXf/rn9tacbPHixXK73fbl0KFDHRseAIAOMj7STU1NSk1NVUVFhQoKCuyzaElyuVxqbGxUTU2N122qq6sVERFhr/nmm29a3e+3335rr2lLUFCQBg4c6HUBAKAnGR3pE4H+/PPPVVhYqLCwMK/9cXFxCggI8HqBWWVlpcrKyhQfHy9JGj9+vNxutz788EN7zQcffCC3222vAQDARD59dXd9fb2++OIL+3pFRYVKS0sVGhqqyMhI/eu//qv27t2r//u//1Nzc7P9HHJoaKgCAwPldDo1Y8YMLVy4UGFhYQoNDdWiRYs0atQo+9XeI0aM0A033KB7771XL730kiTpvvvuU3JyMq/sBgAYzaeR3rNnj66//nr7+oIFCyRJ06dPV0ZGhrZs2SJJuvLKK71u98477yghIUGStGLFCvn7+ys1NVUNDQ2aOHGicnNz5efnZ69/5ZVXNG/ePPtV4CkpKW2+NxsAAJP4NNIJCQmyLKvd/afad0JwcLBycnKUk5PT7prQ0FDl5eV1aEYAAHzF6OekAQDoy4g0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACG8mmk33vvPd10002KjIyUw+HQ5s2bvfZblqWMjAxFRkaqX79+SkhIUHl5udcaj8ejuXPnavDgwRowYIBSUlJ0+PBhrzU1NTVKS0uT0+mU0+lUWlqajh071s1HBwBA5/g00sePH9fo0aO1cuXKNvcvW7ZMy5cv18qVK7V79265XC5NnjxZdXV19pr09HRt2rRJ+fn52rFjh+rr65WcnKzm5mZ7zbRp01RaWqqtW7dq69atKi0tVVpaWrcfHwAAneHvywefMmWKpkyZ0uY+y7KUnZ2tJUuWaOrUqZKk9evXKyIiQhs3btTMmTPldru1du1abdiwQZMmTZIk5eXlKSoqSoWFhUpKStL+/fu1detW7dq1S+PGjZMkrVmzRuPHj9eBAwc0fPjwnjlYAADOkLHPSVdUVKiqqkqJiYn2tqCgIE2YMEHFxcWSpJKSEjU1NXmtiYyMVGxsrL1m586dcjqddqAl6ZprrpHT6bTXtMXj8ai2ttbrAgBATzI20lVVVZKkiIgIr+0RERH2vqqqKgUGBmrQoEGnXBMeHt7q/sPDw+01bcnKyrKfw3Y6nYqKiurU8QAAcKaMjfQJDofD67plWa22nezkNW2t/7n7Wbx4sdxut305dOjQGU4OAEDnGBtpl8slSa3Odqurq+2za5fLpcbGRtXU1JxyzTfffNPq/r/99ttWZ+k/FRQUpIEDB3pdAADoScZGOjo6Wi6XSwUFBfa2xsZGFRUVKT4+XpIUFxengIAArzWVlZUqKyuz14wfP15ut1sffvihveaDDz6Q2+221wAAYCKfvrq7vr5eX3zxhX29oqJCpaWlCg0N1dChQ5Wenq7MzEzFxMQoJiZGmZmZ6t+/v6ZNmyZJcjqdmjFjhhYuXKiwsDCFhoZq0aJFGjVqlP1q7xEjRuiGG27Qvffeq5deekmSdN999yk5OZlXdgMAjObTSO/Zs0fXX3+9fX3BggWSpOnTpys3N1cPPfSQGhoaNGvWLNXU1GjcuHHavn27QkJC7NusWLFC/v7+Sk1NVUNDgyZOnKjc3Fz5+fnZa1555RXNmzfPfhV4SkpKu+/NBgDAFD6NdEJCgizLane/w+FQRkaGMjIy2l0THBysnJwc5eTktLsmNDRUeXl5nRkVAIAeZ+xz0gAA9HVEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQ3Uo0hdddJG+++67VtuPHTumiy66qNNDnfDjjz/qd7/7naKjo9WvXz9ddNFFevzxx9XS0mKvsSxLGRkZioyMVL9+/ZSQkKDy8nKv+/F4PJo7d64GDx6sAQMGKCUlRYcPH+6yOQEA6A4divRXX32l5ubmVts9Ho+OHDnS6aFOeOqpp/Tiiy9q5cqV2r9/v5YtW6ann35aOTk59pply5Zp+fLlWrlypXbv3i2Xy6XJkyerrq7OXpOenq5NmzYpPz9fO3bsUH19vZKTk9s8BgAATOF/Jou3bNli/3nbtm1yOp329ebmZr311lsaNmxYlw23c+dO3XzzzbrxxhslScOGDdOrr76qPXv2SPrHWXR2draWLFmiqVOnSpLWr1+viIgIbdy4UTNnzpTb7dbatWu1YcMGTZo0SZKUl5enqKgoFRYWKikpqcvmBQCgK51RpG+55RZJksPh0PTp0732BQQEaNiwYXr22We7bLjrrrtOL774oj777DNdeuml+vjjj7Vjxw5lZ2dLkioqKlRVVaXExET7NkFBQZowYYKKi4s1c+ZMlZSUqKmpyWtNZGSkYmNjVVxc3G6kPR6PPB6Pfb22trbLjgsAgNNxRpE+8VxwdHS0du/ercGDB3fLUCc8/PDDcrvduuyyy+Tn56fm5mY98cQT+rd/+zdJUlVVlSQpIiLC63YRERH6+uuv7TWBgYEaNGhQqzUnbt+WrKwsPfbYY115OAAAnJEOPSddUVHR7YGWpNdee015eXnauHGj9u7dq/Xr1+uZZ57R+vXrvdY5HA6v65Zltdp2sp9bs3jxYrndbvty6NChjh8IAAAdcEZn0j/11ltv6a233lJ1dbXXq60l6b//+787PZgkPfjgg3rkkUd02223SZJGjRqlr7/+WllZWZo+fbpcLpekf5wtDxkyxL5ddXW1fXbtcrnU2Niompoar7Pp6upqxcfHt/vYQUFBCgoK6pLjAACgIzp0Jv3YY48pMTFRb731lo4ePaqamhqvS1f5/vvvdc453iP6+fl5/djd5XKpoKDA3t/Y2KiioiI7wHFxcQoICPBaU1lZqbKyslNGGgAAX+vQmfSLL76o3NxcpaWldfU8Xm666SY98cQTGjp0qC6//HJ99NFHWr58ue6++25J//gxd3p6ujIzMxUTE6OYmBhlZmaqf//+mjZtmiTJ6XRqxowZWrhwocLCwhQaGqpFixZp1KhR9qu9AQAwUYci3djY2CNnoTk5OXr00Uc1a9YsVVdXKzIyUjNnztTvf/97e81DDz2khoYGzZo1SzU1NRo3bpy2b9+ukJAQe82KFSvk7++v1NRUNTQ0aOLEicrNzZWfn1+3HwMAAB3VoUjfc8892rhxox599NGunsdLSEiIsrOz7bdctcXhcCgjI0MZGRntrgkODlZOTo7XL0EBAMB0HYr0Dz/8oNWrV6uwsFBXXHGFAgICvPYvX768S4YDAKAv61CkP/nkE1155ZWSpLKyMq99P/fWJwAAcHo6FOl33nmnq+cAAAAn4aMqAQAwVIfOpK+//vpT/lj77bff7vBAAADgHzoU6RPPR5/Q1NSk0tJSlZWVtfrgDQAA0DEdivSKFSva3J6RkaH6+vpODQQAAP6hS5+TvuOOO7rs93YDANDXdWmkd+7cqeDg4K68SwAA+qwO/bh76tSpXtcty1JlZaX27NnT7b+FDACAvqJDkXY6nV7XzznnHA0fPlyPP/64EhMTu2QwAAD6ug5Fet26dV09BwAAOEmHIn1CSUmJ9u/fL4fDoZEjR2rMmDFdNRcAAH1ehyJdXV2t2267Te+++67OO+88WZYlt9ut66+/Xvn5+Tr//PO7ek4AAPqcDr26e+7cuaqtrVV5ebn+/ve/q6amRmVlZaqtrdW8efO6ekYAAPqkDp1Jb926VYWFhRoxYoS9beTIkXr++ed54RgAAF2kQ2fSLS0trT5DWpICAgLU0tLS6aEAAEAHI/3rX/9a8+fP19/+9jd725EjR/TAAw9o4sSJXTYcAAB9WYcivXLlStXV1WnYsGG6+OKLdckllyg6Olp1dXXKycnp6hkBAOiTOvScdFRUlPbu3auCggJ9+umnsixLI0eO1KRJk7p6PgAA+qwzOpN+++23NXLkSNXW1kqSJk+erLlz52revHm66qqrdPnll+v999/vlkEBAOhrzijS2dnZuvfeezVw4MBW+5xOp2bOnKnly5d32XAAAPRlZxTpjz/+WDfccEO7+xMTE1VSUtLpoQAAwBlG+ptvvmnzrVcn+Pv769tvv+30UAAA4Awj/U//9E/at29fu/s/+eQTDRkypNNDAQCAM4z0P//zP+v3v/+9fvjhh1b7GhoatHTpUiUnJ3fZcAAA9GVn9Bas3/3ud3r99dd16aWXas6cORo+fLgcDof279+v559/Xs3NzVqyZEl3zQoAQJ9yRpGOiIhQcXGx/v3f/12LFy+WZVmSJIfDoaSkJL3wwguKiIjolkEBAOhrzviXmVx44YX685//rJqaGn3xxReyLEsxMTEaNGhQd8wHAECf1aHfOCZJgwYN0lVXXdWVswAAgJ/o0O/uBgAA3Y9IAwBgKCINAIChiDQAAIYi0gAAGIpIAwBgKCINAIChiDQAAIYi0gAAGIpIAwBgKCINAIChiDQAAIYi0gAAGIpIAwBgKCINAIChiDQAAIYyPtJHjhzRHXfcobCwMPXv319XXnmlSkpK7P2WZSkjI0ORkZHq16+fEhISVF5e7nUfHo9Hc+fO1eDBgzVgwAClpKTo8OHDPX0oAACcEaMjXVNTo2uvvVYBAQF688039de//lXPPvuszjvvPHvNsmXLtHz5cq1cuVK7d++Wy+XS5MmTVVdXZ69JT0/Xpk2blJ+frx07dqi+vl7Jyclqbm72wVEBAHB6/H09wKk89dRTioqK0rp16+xtw4YNs/9sWZays7O1ZMkSTZ06VZK0fv16RUREaOPGjZo5c6bcbrfWrl2rDRs2aNKkSZKkvLw8RUVFqbCwUElJSW0+tsfjkcfjsa/X1tZ2wxECANA+o8+kt2zZorFjx+rWW29VeHi4xowZozVr1tj7KyoqVFVVpcTERHtbUFCQJkyYoOLiYklSSUmJmpqavNZERkYqNjbWXtOWrKwsOZ1O+xIVFdUNRwgAQPuMjvSXX36pVatWKSYmRtu2bdP999+vefPm6eWXX5YkVVVVSZIiIiK8bhcREWHvq6qqUmBgoAYNGtTumrYsXrxYbrfbvhw6dKgrDw0AgJ9l9I+7W1paNHbsWGVmZkqSxowZo/Lycq1atUp33nmnvc7hcHjdzrKsVttO9nNrgoKCFBQU1InpAQDoHKPPpIcMGaKRI0d6bRsxYoQOHjwoSXK5XJLU6oy4urraPrt2uVxqbGxUTU1Nu2sAADCR0ZG+9tprdeDAAa9tn332mS688EJJUnR0tFwulwoKCuz9jY2NKioqUnx8vCQpLi5OAQEBXmsqKytVVlZmrwEAwERG/7j7gQceUHx8vDIzM5WamqoPP/xQq1ev1urVqyX948fc6enpyszMVExMjGJiYpSZman+/ftr2rRpkiSn06kZM2Zo4cKFCgsLU2hoqBYtWqRRo0bZr/YGAMBERkf6qquu0qZNm7R48WI9/vjjio6OVnZ2tm6//XZ7zUMPPaSGhgbNmjVLNTU1GjdunLZv366QkBB7zYoVK+Tv76/U1FQ1NDRo4sSJys3NlZ+fny8OCwCA02J0pCUpOTlZycnJ7e53OBzKyMhQRkZGu2uCg4OVk5OjnJycbpgQAIDuYfRz0gAA9GVEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQ/WqSGdlZcnhcCg9Pd3eZlmWMjIyFBkZqX79+ikhIUHl5eVet/N4PJo7d64GDx6sAQMGKCUlRYcPH+7h6QEAODO9JtK7d+/W6tWrdcUVV3htX7ZsmZYvX66VK1dq9+7dcrlcmjx5surq6uw16enp2rRpk/Lz87Vjxw7V19crOTlZzc3NPX0YAACctl4R6fr6et1+++1as2aNBg0aZG+3LEvZ2dlasmSJpk6dqtjYWK1fv17ff/+9Nm7cKElyu91au3atnn32WU2aNEljxoxRXl6e9u3bp8LCwnYf0+PxqLa21usCAEBP6hWRnj17tm688UZNmjTJa3tFRYWqqqqUmJhobwsKCtKECRNUXFwsSSopKVFTU5PXmsjISMXGxtpr2pKVlSWn02lfoqKiuvioAAA4NeMjnZ+fr5KSEmVlZbXaV1VVJUmKiIjw2h4REWHvq6qqUmBgoNcZ+Mlr2rJ48WK53W77cujQoc4eCgAAZ8Tf1wOcyqFDhzR//nxt375dwcHB7a5zOBxe1y3LarXtZD+3JigoSEFBQWc2MAAAXcjoM+mSkhJVV1crLi5O/v7+8vf3V1FRkZ577jn5+/vbZ9AnnxFXV1fb+1wulxobG1VTU9PuGgAATGR0pCdOnKh9+/aptLTUvowdO1a33367SktLddFFF8nlcqmgoMC+TWNjo4qKihQfHy9JiouLU0BAgNeayspKlZWV2WsAADCR0T/uDgkJUWxsrNe2AQMGKCwszN6enp6uzMxMxcTEKCYmRpmZmerfv7+mTZsmSXI6nZoxY4YWLlyosLAwhYaGatGiRRo1alSrF6IBAGASoyN9Oh566CE1NDRo1qxZqqmp0bhx47R9+3aFhITYa1asWCF/f3+lpqaqoaFBEydOVG5urvz8/Hw4OQAAp9brIv3uu+96XXc4HMrIyFBGRka7twkODlZOTo5ycnK6dzgAALqQ0c9JAwDQlxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxFpAAAMRaQBADAUkQYAwFBEGgAAQxkd6aysLF111VUKCQlReHi4brnlFh04cMBrjWVZysjIUGRkpPr166eEhASVl5d7rfF4PJo7d64GDx6sAQMGKCUlRYcPH+7JQwEA4IwZHemioiLNnj1bu3btUkFBgX788UclJibq+PHj9pply5Zp+fLlWrlypXbv3i2Xy6XJkyerrq7OXpOenq5NmzYpPz9fO3bsUH19vZKTk9Xc3OyLwwIA4LT4+3qAU9m6davX9XXr1ik8PFwlJSX61a9+JcuylJ2drSVLlmjq1KmSpPXr1ysiIkIbN27UzJkz5Xa7tXbtWm3YsEGTJk2SJOXl5SkqKkqFhYVKSkrq8eMCAOB0GH0mfTK32y1JCg0NlSRVVFSoqqpKiYmJ9pqgoCBNmDBBxcXFkqSSkhI1NTV5rYmMjFRsbKy9pi0ej0e1tbVeFwAAelKvibRlWVqwYIGuu+46xcbGSpKqqqokSREREV5rIyIi7H1VVVUKDAzUoEGD2l3TlqysLDmdTvsSFRXVlYcDAMDP6jWRnjNnjj755BO9+uqrrfY5HA6v65Zltdp2sp9bs3jxYrndbvty6NChjg0OAEAH9YpIz507V1u2bNE777yjCy64wN7ucrkkqdUZcXV1tX127XK51NjYqJqamnbXtCUoKEgDBw70ugAA0JOMjrRlWZozZ45ef/11vf3224qOjvbaHx0dLZfLpYKCAntbY2OjioqKFB8fL0mKi4tTQECA15rKykqVlZXZawAAMJHRr+6ePXu2Nm7cqD/96U8KCQmxz5idTqf69esnh8Oh9PR0ZWZmKiYmRjExMcrMzFT//v01bdo0e+2MGTO0cOFChYWFKTQ0VIsWLdKoUaPsV3sDAGAioyO9atUqSVJCQoLX9nXr1umuu+6SJD300ENqaGjQrFmzVFNTo3Hjxmn79u0KCQmx169YsUL+/v5KTU1VQ0ODJk6cqNzcXPn5+fXUoQAAcMaMjrRlWT+7xuFwKCMjQxkZGe2uCQ4OVk5OjnJycrpwOgAAupfRz0kDANCXEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAxFpAEAMBSRBgDAUEQaAABDEWkAAAzVpyL9wgsvKDo6WsHBwYqLi9P777/v65EAAGhXn4n0a6+9pvT0dC1ZskQfffSRfvnLX2rKlCk6ePCgr0cDAKBNfSbSy5cv14wZM3TPPfdoxIgRys7OVlRUlFatWuXr0QAAaJO/rwfoCY2NjSopKdEjjzzitT0xMVHFxcVt3sbj8cjj8djX3W63JKm2trZTs9TX10uS/v71Af3oaejUfZ2stuofPxUoKSmxH6erHDhwQBJz/xRzt+2cc85RS0tLl99vb/076a1zS7139p6Yu76+vtM9kKSQkBA5HI72F1h9wJEjRyxJ1l/+8hev7U888YR16aWXtnmbpUuXWpK4cOHChQuXbru43e5T9qtPnEmfcPJ3K5ZltfsdzOLFi7VgwQL7ektLi/7+978rLCzs1N/1/Iza2lpFRUXp0KFDGjhwYIfvxyRn2zGdbccjcUy9wdl2PBLHdDpCQkJOub9PRHrw4MHy8/NTVVWV1/bq6mpFRES0eZugoCAFBQV5bTvvvPO6bKaBAweeNV+0J5xtx3S2HY/EMfUGZ9vxSBxTZ/SJF44FBgYqLi5OBQUFXtsLCgoUHx/vo6kAADi1PnEmLUkLFixQWlqaxo4dq/Hjx2v16tU6ePCg7r//fl+PBgBAm/pMpH/zm9/ou+++0+OPP67KykrFxsbqz3/+sy688MIenSMoKEhLly5t9aP03uxsO6az7Xgkjqk3ONuOR+KYuoLDsiyrRx4JAACckT7xnDQAAL0RkQYAwFBEGgAAQxFpAAAMRaR72Nn0cZnvvfeebrrpJkVGRsrhcGjz5s2+HqlTsrKydNVVVykkJETh4eG65ZZb7N8B3FutWrVKV1xxhf2LF8aPH68333zT12N1maysLDkcDqWnp/t6lA7LyMiQw+HwurhcLl+P1WlHjhzRHXfcobCwMPXv319XXnmlSkpKfD1WhwwbNqzVv5HD4dDs2bO7/bGJdA862z4u8/jx4xo9erRWrlzp61G6RFFRkWbPnq1du3apoKBAP/74oxITE3X8+HFfj9ZhF1xwgZ588knt2bNHe/bs0a9//WvdfPPNKi8v9/VonbZ7926tXr1aV1xxha9H6bTLL79clZWV9mXfvn2+HqlTampqdO211yogIEBvvvmm/vrXv+rZZ5/t0t/a2JN2797t9e9z4hdj3Xrrrd3/4F3zERY4HVdffbV1//33e2277LLLrEceecRHE3UdSdamTZt8PUaXqq6utiRZRUVFvh6lSw0aNMj6r//6L1+P0Sl1dXVWTEyMVVBQYE2YMMGaP3++r0fqsKVLl1qjR4/29Rhd6uGHH7auu+46X4/RbebPn29dfPHFVktLS7c/FmfSPeTEx2UmJiZ6bT/Vx2XCt058PGloaKiPJ+kazc3Nys/P1/HjxzV+/Hhfj9Mps2fP1o033qhJkyb5epQu8fnnnysyMlLR0dG67bbb9OWXX/p6pE7ZsmWLxo4dq1tvvVXh4eEaM2aM1qxZ4+uxukRjY6Py8vJ09913d+rDlk4Xke4hR48eVXNzc6sP9IiIiGj1wR/wPcuytGDBAl133XWKjY319Tidsm/fPp177rkKCgrS/fffr02bNmnkyJG+HqvD8vPzVVJSoqysLF+P0iXGjRunl19+Wdu2bdOaNWtUVVWl+Ph4fffdd74ercO+/PJLrVq1SjExMdq2bZvuv/9+zZs3Ty+//LKvR+u0zZs369ixY7rrrrt65PH6zK8FNcWZfFwmfGfOnDn65JNPtGPHDl+P0mnDhw9XaWmpjh07pv/93//V9OnTVVRU1CtDfejQIc2fP1/bt29XcHCwr8fpElOmTLH/PGrUKI0fP14XX3yx1q9f7/Vxub1JS0uLxo4dq8zMTEnSmDFjVF5erlWrVunOO+/08XSds3btWk2ZMkWRkZE98nicSfeQjnxcJnxj7ty52rJli9555x1dcMEFvh6n0wIDA3XJJZdo7NixysrK0ujRo/XHP/7R12N1SElJiaqrqxUXFyd/f3/5+/urqKhIzz33nPz9/dXc3OzrETttwIABGjVqlD7//HNfj9JhQ4YMafVN4IgRI3rti2RP+Prrr1VYWKh77rmnxx6TSPcQPi7TfJZlac6cOXr99df19ttvKzo62tcjdQvLsuTxeHw9RodMnDhR+/btU2lpqX0ZO3asbr/9dpWWlsrPz8/XI3aax+PR/v37NWTIEF+P0mHXXnttq7cvfvbZZz3+gUZdbd26dQoPD9eNN97YY4/Jj7t70Nn2cZn19fX64osv7OsVFRUqLS1VaGiohg4d6sPJOmb27NnauHGj/vSnPykkJMT+qYfT6VS/fv18PF3H/Md//IemTJmiqKgo1dXVKT8/X++++662bt3q69E6JCQkpNVrBAYMGKCwsLBe+9qBRYsW6aabbtLQoUNVXV2t//zP/1Rtba2mT5/u69E67IEHHlB8fLwyMzOVmpqqDz/8UKtXr9bq1at9PVqHtbS0aN26dZo+fbr8/Xswnd3++nF4ef75560LL7zQCgwMtH7xi1/06rf3vPPOO5akVpfp06f7erQOaetYJFnr1q3z9Wgddvfdd9tfb+eff741ceJEa/v27b4eq0v19rdg/eY3v7GGDBliBQQEWJGRkdbUqVOt8vJyX4/VaW+88YYVGxtrBQUFWZdddpm1evVqX4/UKdu2bbMkWQcOHOjRx+WjKgEAMBTPSQMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSANoJSEhQenp6ae19t1335XD4dCxY8c69ZjDhg1TdnZ2p+4DONsQaQAADEWkAQAwFJEGcEp5eXkaO3asQkJC5HK5NG3aNFVXV7da95e//EWjR49WcHCwxo0bp3379nntLy4u1q9+9Sv169dPUVFRmjdvno4fP95ThwH0SkQawCk1NjbqD3/4gz7++GNt3rxZFRUVuuuuu1qte/DBB/XMM89o9+7dCg8PV0pKipqamiRJ+/btU1JSkqZOnapPPvlEr732mnbs2KE5c+b08NEAvQufJw3glO6++277zxdddJGee+45XX311aqvr9e5555r71u6dKkmT54sSVq/fr0uuOACbdq0SampqXr66ac1bdo0+8VoMTExeu655zRhwgStWrVKwcHBPXpMQG/BmTSAU/roo490880368ILL1RISIgSEhIkSQcPHvRaN378ePvPoaGhGj58uPbv3y9JKikpUW5urs4991z7kpSUpJaWFlVUVPTYsQC9DWfSANp1/PhxJSYmKjExUXl5eTr//PN18OBBJSUlqbGx8Wdv73A4JEktLS2aOXOm5s2b12rN0KFDu3xu4GxBpAG069NPP9XRo0f15JNPKioqSpK0Z8+eNtfu2rXLDm5NTY0+++wzXXbZZZKkX/ziFyovL9cll1zSM4MDZwl+3A2gXUOHDlVgYKBycnL05ZdfasuWLfrDH/7Q5trHH39cb731lsrKynTXXXdp8ODBuuWWWyRJDz/8sHbu3KnZs2ertLRUn3/+ubZs2aK5c+f24NEAvQ+RBtCu888/X7m5ufqf//kfjRw5Uk8++aSeeeaZNtc++eSTmj9/vuLi4lRZWaktW7YoMDBQknTFFVeoqKhIn3/+uX75y19qzJgxevTRRzVkyJCePByg13FYlmX5eggAANAaZ9IAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAoYg0AACGItIAABiKSAMAYCgiDQCAof4flklyEIlTX7QAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.displot(dataset.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "912b572b-1ad6-4e90-8f4e-20574d917356",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912b572b-1ad6-4e90-8f4e-20574d917356",
        "outputId": "9f9655c5-a54f-4d72-df8d-b1ed2a51605e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1cba2367e30>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a60d4a3a-bc79-47f1-9040-06f8e4140b46",
      "metadata": {
        "id": "a60d4a3a-bc79-47f1-9040-06f8e4140b46"
      },
      "outputs": [],
      "source": [
        "class ComidasDataset(Dataset):\n",
        "    def __init__(self, annotations_file : pd.DataFrame, transform=None, target_transform=None):\n",
        "        self.img_labels = annotations_file\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        img_path = self.img_labels.iloc[idx, 0]\n",
        "        img_path = dir_imagenes + os.path.basename(img_path)\n",
        "        image = Img.open(img_path).convert('RGB')\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9412a324-39dc-42b8-8c6a-36b5c713ce68",
      "metadata": {
        "id": "9412a324-39dc-42b8-8c6a-36b5c713ce68"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(400,400)),\n",
        "    transforms.RandomResizedCrop(300),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1aff671c-35b1-4d71-bb27-06fc4588d8d9",
      "metadata": {
        "id": "1aff671c-35b1-4d71-bb27-06fc4588d8d9"
      },
      "outputs": [],
      "source": [
        "dat_train = ComidasDataset( train_dataset, transform=transform )\n",
        "dat_test = ComidasDataset( val_dataset, transform=transform )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bfeff77d-858e-41d1-acf2-7dc2290b0717",
      "metadata": {
        "id": "bfeff77d-858e-41d1-acf2-7dc2290b0717"
      },
      "outputs": [],
      "source": [
        "\"\"\" Muy importante:\n",
        "    Aqui se decide el batch size, que despues se incluye en la capa lineal\n",
        "\"\"\"\n",
        "train_dataloader = DataLoader(dat_train, batch_size=10, shuffle=True)\n",
        "test_dataloader = DataLoader(dat_test, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "63539039-3c6a-4cbd-a1a1-91c7d197b213",
      "metadata": {
        "id": "63539039-3c6a-4cbd-a1a1-91c7d197b213"
      },
      "outputs": [],
      "source": [
        "# base de rnn sacada de https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model\n",
        "\n",
        "\n",
        "# Define a convolution neural network\n",
        "# Variotanal autoencoder ?\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        # se han usadp kernels de 3x3 para no complicar los calculos de dimensiones\n",
        "        \n",
        "        # in_channes = 3 colores (RGB)\n",
        "        # out_channes = salidad de la convolucion (3*4)? \n",
        "        # kernes_size = tamaño de la cuadricula de convolucion\n",
        "        # stride = pasos que se mueve la convolucion\n",
        "        # padding = bode generado para no perder features en los extremos\n",
        "        # Sacal un featuremap de 12 canales: [batchx12x512x512}\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(12)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(12)\n",
        "        # max pool reduce las dimensiones del featuremp\n",
        "        # salida = [batchx12x64x64]\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # salida = [batchx24x64x64]\n",
        "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(24)\n",
        "        \n",
        "        # salida = [batchx24x64x64]\n",
        "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(24)\n",
        "\n",
        "        #test, no se usa en el resultado\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        self.fc2 = nn.Linear(24*150*150, 128)\n",
        "        self.fc1 = nn.Linear(128, 8)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, input):\n",
        "        # fase 1 se pasa el output de la convolucion a BatchNorm y a ReLu\n",
        "        output = F.relu(self.bn1(self.conv1(input)))\n",
        "        output = F.relu(self.bn2(self.conv2(output)))\n",
        "        output = self.pool(output)\n",
        "        output = F.relu(self.bn4(self.conv4(output)))\n",
        "        output = F.relu(self.bn5(self.conv5(output)))\n",
        "        #print(output.size())\n",
        "        output = output.view(-1, 24*150*150)\n",
        "        #print(output.size())\n",
        "        output = self.fc2(output)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Instantiate a neural network model \n",
        "model = Network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "14b79b8a-7713-42c2-8f65-72f4075eda08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14b79b8a-7713-42c2-8f65-72f4075eda08",
        "outputId": "769f94e1-62b2-4d9d-dd23-d64350b6e7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0: --> NVIDIA GeForce RTX 3050 Ti Laptop GPU device\n"
          ]
        }
      ],
      "source": [
        "device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}: --> {torch.cuda.get_device_name(device)} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "abc4b0bf-78f5-4a29-babe-9b5e4252c072",
      "metadata": {
        "id": "abc4b0bf-78f5-4a29-babe-9b5e4252c072"
      },
      "outputs": [],
      "source": [
        "# Funciones base sacadas de https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    best_accuracy = 0.0\n",
        "    \n",
        "    # Define your execution device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # device = xm.xla_device()\n",
        "    print(\"The model will be running on\", device, \"device\")\n",
        "    # Convert model parameters and buffers to CPU or Cuda\n",
        "    model = model.to(device)\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    for batch, (images, label) in enumerate(dataloader):\n",
        "        # get the inputs\n",
        "        #X = Variable(torch.tensor(images))\n",
        "        images = images.to(device)\n",
        "        y = Variable(label.to(device))\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(images)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(batch)\n",
        "        # print cada 2 batches = 2 * 1024 = 2048 imagenes\n",
        "        if batch % 5 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(images)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Define your execution device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # device = xm.xla_device()\n",
        "\n",
        "    # matriz de confusion\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            '''\n",
        "            output = (torch.max(torch.exp(pred), 1)[1]).data.cpu().numpy()\n",
        "\n",
        "            y_pred.extend(output) # Save Prediction\n",
        "        \n",
        "            labels = labels.data.cpu().numpy()\n",
        "            y_true.extend(labels) # Save Truth\n",
        "            '''\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    '''\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion\")\n",
        "    print(conf_matrix)\n",
        "    '''\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "lOFKujTChKPg",
      "metadata": {
        "id": "lOFKujTChKPg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilizando: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model.load_state_dict(torch.load(drive+f\"model_13896134932304.pth\"))\n",
        "\n",
        "print(f\"Utilizando: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cff90a72-c3bf-4219-a98f-3dc5265de323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "id": "cff90a72-c3bf-4219-a98f-3dc5265de323",
        "outputId": "be6340fa-1e09-4716-db9f-e1c34399e038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.030154  [   10/ 7840]\n",
            "loss: 4806.704590  [   60/ 7840]\n",
            "loss: 1416.851318  [  110/ 7840]\n",
            "loss: 3139.115967  [  160/ 7840]\n",
            "loss: 1082.087158  [  210/ 7840]\n",
            "loss: 688.424316  [  260/ 7840]\n",
            "loss: 647.390259  [  310/ 7840]\n",
            "loss: 397.007385  [  360/ 7840]\n",
            "loss: 328.859192  [  410/ 7840]\n",
            "loss: 192.521210  [  460/ 7840]\n",
            "loss: 152.413574  [  510/ 7840]\n",
            "loss: 161.308563  [  560/ 7840]\n",
            "loss: 150.582397  [  610/ 7840]\n",
            "loss: 72.196426  [  660/ 7840]\n",
            "loss: 67.618210  [  710/ 7840]\n",
            "loss: 54.287666  [  760/ 7840]\n",
            "loss: 35.813454  [  810/ 7840]\n",
            "loss: 51.015305  [  860/ 7840]\n",
            "loss: 47.179546  [  910/ 7840]\n",
            "loss: 19.436604  [  960/ 7840]\n",
            "loss: 30.273865  [ 1010/ 7840]\n",
            "loss: 25.021097  [ 1060/ 7840]\n",
            "loss: 37.751823  [ 1110/ 7840]\n",
            "loss: 12.527838  [ 1160/ 7840]\n",
            "loss: 10.499110  [ 1210/ 7840]\n",
            "loss: 15.675984  [ 1260/ 7840]\n",
            "loss: 5.272470  [ 1310/ 7840]\n",
            "loss: 3.850621  [ 1360/ 7840]\n",
            "loss: 7.084550  [ 1410/ 7840]\n",
            "loss: 8.078863  [ 1460/ 7840]\n",
            "loss: 5.710891  [ 1510/ 7840]\n",
            "loss: 5.977996  [ 1560/ 7840]\n",
            "loss: 6.968942  [ 1610/ 7840]\n",
            "loss: 6.291040  [ 1660/ 7840]\n",
            "loss: 3.914844  [ 1710/ 7840]\n",
            "loss: 8.968042  [ 1760/ 7840]\n",
            "loss: 4.713239  [ 1810/ 7840]\n",
            "loss: 4.387146  [ 1860/ 7840]\n",
            "loss: 3.982016  [ 1910/ 7840]\n",
            "loss: 2.375796  [ 1960/ 7840]\n",
            "loss: 2.776020  [ 2010/ 7840]\n",
            "loss: 6.305533  [ 2060/ 7840]\n",
            "loss: 5.133416  [ 2110/ 7840]\n",
            "loss: 2.570216  [ 2160/ 7840]\n",
            "loss: 4.403741  [ 2210/ 7840]\n",
            "loss: 2.183469  [ 2260/ 7840]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mario\\anaconda3\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 3.310760  [ 2310/ 7840]\n",
            "loss: 3.787205  [ 2360/ 7840]\n",
            "loss: 2.965176  [ 2410/ 7840]\n",
            "loss: 4.237495  [ 2460/ 7840]\n",
            "loss: 2.619063  [ 2510/ 7840]\n",
            "loss: 2.376907  [ 2560/ 7840]\n",
            "loss: 3.495225  [ 2610/ 7840]\n",
            "loss: 2.427114  [ 2660/ 7840]\n",
            "loss: 3.457490  [ 2710/ 7840]\n",
            "loss: 2.736624  [ 2760/ 7840]\n",
            "loss: 2.149031  [ 2810/ 7840]\n",
            "loss: 3.082492  [ 2860/ 7840]\n",
            "loss: 2.041179  [ 2910/ 7840]\n",
            "loss: 2.895219  [ 2960/ 7840]\n",
            "loss: 2.406996  [ 3010/ 7840]\n",
            "loss: 2.006307  [ 3060/ 7840]\n",
            "loss: 2.008457  [ 3110/ 7840]\n",
            "loss: 2.370728  [ 3160/ 7840]\n",
            "loss: 1.873268  [ 3210/ 7840]\n",
            "loss: 3.611030  [ 3260/ 7840]\n",
            "loss: 2.118443  [ 3310/ 7840]\n",
            "loss: 2.697916  [ 3360/ 7840]\n",
            "loss: 2.102488  [ 3410/ 7840]\n",
            "loss: 2.157150  [ 3460/ 7840]\n",
            "loss: 2.411765  [ 3510/ 7840]\n",
            "loss: 2.092116  [ 3560/ 7840]\n",
            "loss: 2.038020  [ 3610/ 7840]\n",
            "loss: 2.013723  [ 3660/ 7840]\n",
            "loss: 2.203525  [ 3710/ 7840]\n",
            "loss: 2.738631  [ 3760/ 7840]\n",
            "loss: 1.943472  [ 3810/ 7840]\n",
            "loss: 2.155337  [ 3860/ 7840]\n",
            "loss: 2.171394  [ 3910/ 7840]\n",
            "loss: 1.929991  [ 3960/ 7840]\n",
            "loss: 2.283862  [ 4010/ 7840]\n",
            "loss: 2.106553  [ 4060/ 7840]\n",
            "loss: 1.862619  [ 4110/ 7840]\n",
            "loss: 2.018914  [ 4160/ 7840]\n",
            "loss: 2.660753  [ 4210/ 7840]\n",
            "loss: 2.314089  [ 4260/ 7840]\n",
            "loss: 2.004391  [ 4310/ 7840]\n",
            "loss: 2.409715  [ 4360/ 7840]\n",
            "loss: 2.070508  [ 4410/ 7840]\n",
            "loss: 1.988620  [ 4460/ 7840]\n",
            "loss: 2.040858  [ 4510/ 7840]\n",
            "loss: 1.954849  [ 4560/ 7840]\n",
            "loss: 2.332025  [ 4610/ 7840]\n",
            "loss: 2.201853  [ 4660/ 7840]\n",
            "loss: 2.146048  [ 4710/ 7840]\n",
            "loss: 2.317677  [ 4760/ 7840]\n",
            "loss: 2.072445  [ 4810/ 7840]\n",
            "loss: 2.144718  [ 4860/ 7840]\n",
            "loss: 2.243802  [ 4910/ 7840]\n",
            "loss: 1.936797  [ 4960/ 7840]\n",
            "loss: 2.069964  [ 5010/ 7840]\n",
            "loss: 2.316284  [ 5060/ 7840]\n",
            "loss: 2.021371  [ 5110/ 7840]\n",
            "loss: 2.176286  [ 5160/ 7840]\n",
            "loss: 2.115284  [ 5210/ 7840]\n",
            "loss: 2.001977  [ 5260/ 7840]\n",
            "loss: 2.133330  [ 5310/ 7840]\n",
            "loss: 2.056588  [ 5360/ 7840]\n",
            "loss: 2.089543  [ 5410/ 7840]\n",
            "loss: 1.995816  [ 5460/ 7840]\n",
            "loss: 1.893365  [ 5510/ 7840]\n",
            "loss: 2.238199  [ 5560/ 7840]\n",
            "loss: 2.217861  [ 5610/ 7840]\n",
            "loss: 2.267366  [ 5660/ 7840]\n",
            "loss: 2.213535  [ 5710/ 7840]\n",
            "loss: 2.069409  [ 5760/ 7840]\n",
            "loss: 2.303122  [ 5810/ 7840]\n",
            "loss: 2.104796  [ 5860/ 7840]\n",
            "loss: 2.138210  [ 5910/ 7840]\n",
            "loss: 2.341300  [ 5960/ 7840]\n",
            "loss: 2.035929  [ 6010/ 7840]\n",
            "loss: 2.335682  [ 6060/ 7840]\n",
            "loss: 2.374091  [ 6110/ 7840]\n",
            "loss: 2.168923  [ 6160/ 7840]\n",
            "loss: 2.482888  [ 6210/ 7840]\n",
            "loss: 2.002553  [ 6260/ 7840]\n",
            "loss: 2.035819  [ 6310/ 7840]\n",
            "loss: 2.052602  [ 6360/ 7840]\n",
            "loss: 1.912132  [ 6410/ 7840]\n",
            "loss: 2.194007  [ 6460/ 7840]\n",
            "loss: 2.051190  [ 6510/ 7840]\n",
            "loss: 2.207712  [ 6560/ 7840]\n",
            "loss: 2.100722  [ 6610/ 7840]\n",
            "loss: 2.010235  [ 6660/ 7840]\n",
            "loss: 2.160304  [ 6710/ 7840]\n",
            "loss: 2.111978  [ 6760/ 7840]\n",
            "loss: 2.075351  [ 6810/ 7840]\n",
            "loss: 2.241585  [ 6860/ 7840]\n",
            "loss: 2.114098  [ 6910/ 7840]\n",
            "loss: 2.167236  [ 6960/ 7840]\n",
            "loss: 1.899783  [ 7010/ 7840]\n",
            "loss: 1.945122  [ 7060/ 7840]\n",
            "loss: 2.025058  [ 7110/ 7840]\n",
            "loss: 2.020822  [ 7160/ 7840]\n",
            "loss: 2.104898  [ 7210/ 7840]\n",
            "loss: 1.974276  [ 7260/ 7840]\n",
            "loss: 2.016416  [ 7310/ 7840]\n",
            "loss: 2.058929  [ 7360/ 7840]\n",
            "loss: 1.910822  [ 7410/ 7840]\n",
            "loss: 1.921862  [ 7460/ 7840]\n",
            "loss: 2.015921  [ 7510/ 7840]\n",
            "loss: 2.058753  [ 7560/ 7840]\n",
            "loss: 2.112787  [ 7610/ 7840]\n",
            "loss: 2.244086  [ 7660/ 7840]\n",
            "loss: 2.189250  [ 7710/ 7840]\n",
            "loss: 2.334718  [ 7760/ 7840]\n",
            "loss: 2.080190  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 18.8%, Avg loss: 2.065166 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.048101  [   10/ 7840]\n",
            "loss: 1.898474  [   60/ 7840]\n",
            "loss: 2.054889  [  110/ 7840]\n",
            "loss: 2.554883  [  160/ 7840]\n",
            "loss: 2.221042  [  210/ 7840]\n",
            "loss: 2.233095  [  260/ 7840]\n",
            "loss: 1.973570  [  310/ 7840]\n",
            "loss: 1.950151  [  360/ 7840]\n",
            "loss: 2.169614  [  410/ 7840]\n",
            "loss: 2.040006  [  460/ 7840]\n",
            "loss: 1.978436  [  510/ 7840]\n",
            "loss: 2.030210  [  560/ 7840]\n",
            "loss: 2.121473  [  610/ 7840]\n",
            "loss: 2.137101  [  660/ 7840]\n",
            "loss: 2.128471  [  710/ 7840]\n",
            "loss: 2.082259  [  760/ 7840]\n",
            "loss: 2.044773  [  810/ 7840]\n",
            "loss: 1.978006  [  860/ 7840]\n",
            "loss: 2.147047  [  910/ 7840]\n",
            "loss: 2.080197  [  960/ 7840]\n",
            "loss: 2.108294  [ 1010/ 7840]\n",
            "loss: 2.122582  [ 1060/ 7840]\n",
            "loss: 2.074054  [ 1110/ 7840]\n",
            "loss: 2.142707  [ 1160/ 7840]\n",
            "loss: 1.844785  [ 1210/ 7840]\n",
            "loss: 2.111220  [ 1260/ 7840]\n",
            "loss: 2.539696  [ 1310/ 7840]\n",
            "loss: 2.021070  [ 1360/ 7840]\n",
            "loss: 2.115832  [ 1410/ 7840]\n",
            "loss: 2.081420  [ 1460/ 7840]\n",
            "loss: 2.038165  [ 1510/ 7840]\n",
            "loss: 2.052908  [ 1560/ 7840]\n",
            "loss: 2.012227  [ 1610/ 7840]\n",
            "loss: 2.408641  [ 1660/ 7840]\n",
            "loss: 2.291589  [ 1710/ 7840]\n",
            "loss: 2.046250  [ 1760/ 7840]\n",
            "loss: 2.145224  [ 1810/ 7840]\n",
            "loss: 2.184631  [ 1860/ 7840]\n",
            "loss: 1.906062  [ 1910/ 7840]\n",
            "loss: 2.139337  [ 1960/ 7840]\n",
            "loss: 2.003729  [ 2010/ 7840]\n",
            "loss: 2.064333  [ 2060/ 7840]\n",
            "loss: 1.971361  [ 2110/ 7840]\n",
            "loss: 2.031852  [ 2160/ 7840]\n",
            "loss: 2.082945  [ 2210/ 7840]\n",
            "loss: 2.183500  [ 2260/ 7840]\n",
            "loss: 1.993736  [ 2310/ 7840]\n",
            "loss: 2.142079  [ 2360/ 7840]\n",
            "loss: 1.878991  [ 2410/ 7840]\n",
            "loss: 2.054234  [ 2460/ 7840]\n",
            "loss: 2.025498  [ 2510/ 7840]\n",
            "loss: 2.016946  [ 2560/ 7840]\n",
            "loss: 1.957629  [ 2610/ 7840]\n",
            "loss: 2.223732  [ 2660/ 7840]\n",
            "loss: 1.963780  [ 2710/ 7840]\n",
            "loss: 1.843360  [ 2760/ 7840]\n",
            "loss: 2.040124  [ 2810/ 7840]\n",
            "loss: 2.039007  [ 2860/ 7840]\n",
            "loss: 1.955541  [ 2910/ 7840]\n",
            "loss: 2.073968  [ 2960/ 7840]\n",
            "loss: 2.157539  [ 3010/ 7840]\n",
            "loss: 1.931174  [ 3060/ 7840]\n",
            "loss: 2.054159  [ 3110/ 7840]\n",
            "loss: 2.112031  [ 3160/ 7840]\n",
            "loss: 1.944293  [ 3210/ 7840]\n",
            "loss: 2.098967  [ 3260/ 7840]\n",
            "loss: 2.030197  [ 3310/ 7840]\n",
            "loss: 1.983481  [ 3360/ 7840]\n",
            "loss: 2.059098  [ 3410/ 7840]\n",
            "loss: 2.067588  [ 3460/ 7840]\n",
            "loss: 2.058881  [ 3510/ 7840]\n",
            "loss: 2.082620  [ 3560/ 7840]\n",
            "loss: 2.098523  [ 3610/ 7840]\n",
            "loss: 2.161221  [ 3660/ 7840]\n",
            "loss: 2.150414  [ 3710/ 7840]\n",
            "loss: 1.973695  [ 3760/ 7840]\n",
            "loss: 1.981891  [ 3810/ 7840]\n",
            "loss: 2.115477  [ 3860/ 7840]\n",
            "loss: 3.403903  [ 3910/ 7840]\n",
            "loss: 2.023390  [ 3960/ 7840]\n",
            "loss: 2.136925  [ 4010/ 7840]\n",
            "loss: 1.926306  [ 4060/ 7840]\n",
            "loss: 1.857292  [ 4110/ 7840]\n",
            "loss: 1.908025  [ 4160/ 7840]\n",
            "loss: 2.238818  [ 4210/ 7840]\n",
            "loss: 2.027711  [ 4260/ 7840]\n",
            "loss: 1.918641  [ 4310/ 7840]\n",
            "loss: 1.891917  [ 4360/ 7840]\n",
            "loss: 5.221158  [ 4410/ 7840]\n",
            "loss: 2.037176  [ 4460/ 7840]\n",
            "loss: 1.955132  [ 4510/ 7840]\n",
            "loss: 2.109981  [ 4560/ 7840]\n",
            "loss: 1.883684  [ 4610/ 7840]\n",
            "loss: 1.819233  [ 4660/ 7840]\n",
            "loss: 2.166634  [ 4710/ 7840]\n",
            "loss: 1.934590  [ 4760/ 7840]\n",
            "loss: 1.855565  [ 4810/ 7840]\n",
            "loss: 2.224940  [ 4860/ 7840]\n",
            "loss: 2.053781  [ 4910/ 7840]\n",
            "loss: 2.047516  [ 4960/ 7840]\n",
            "loss: 1.975537  [ 5010/ 7840]\n",
            "loss: 2.039694  [ 5060/ 7840]\n",
            "loss: 2.029020  [ 5110/ 7840]\n",
            "loss: 2.096361  [ 5160/ 7840]\n",
            "loss: 1.966488  [ 5210/ 7840]\n",
            "loss: 2.103034  [ 5260/ 7840]\n",
            "loss: 1.944354  [ 5310/ 7840]\n",
            "loss: 2.147464  [ 5360/ 7840]\n",
            "loss: 2.057895  [ 5410/ 7840]\n",
            "loss: 2.137103  [ 5460/ 7840]\n",
            "loss: 2.073961  [ 5510/ 7840]\n",
            "loss: 2.082684  [ 5560/ 7840]\n",
            "loss: 2.005618  [ 5610/ 7840]\n",
            "loss: 2.000118  [ 5660/ 7840]\n",
            "loss: 1.594977  [ 5710/ 7840]\n",
            "loss: 2.016671  [ 5760/ 7840]\n",
            "loss: 2.094605  [ 5810/ 7840]\n",
            "loss: 1.871962  [ 5860/ 7840]\n",
            "loss: 2.105873  [ 5910/ 7840]\n",
            "loss: 2.153357  [ 5960/ 7840]\n",
            "loss: 2.052979  [ 6010/ 7840]\n",
            "loss: 2.087439  [ 6060/ 7840]\n",
            "loss: 2.035185  [ 6110/ 7840]\n",
            "loss: 2.094197  [ 6160/ 7840]\n",
            "loss: 2.087233  [ 6210/ 7840]\n",
            "loss: 1.704038  [ 6260/ 7840]\n",
            "loss: 2.062490  [ 6310/ 7840]\n",
            "loss: 2.013142  [ 6360/ 7840]\n",
            "loss: 2.012866  [ 6410/ 7840]\n",
            "loss: 2.053202  [ 6460/ 7840]\n",
            "loss: 2.059127  [ 6510/ 7840]\n",
            "loss: 1.918959  [ 6560/ 7840]\n",
            "loss: 1.863608  [ 6610/ 7840]\n",
            "loss: 1.929308  [ 6660/ 7840]\n",
            "loss: 2.017517  [ 6710/ 7840]\n",
            "loss: 2.036350  [ 6760/ 7840]\n",
            "loss: 2.006544  [ 6810/ 7840]\n",
            "loss: 1.939253  [ 6860/ 7840]\n",
            "loss: 1.858750  [ 6910/ 7840]\n",
            "loss: 2.028532  [ 6960/ 7840]\n",
            "loss: 2.183325  [ 7010/ 7840]\n",
            "loss: 1.879432  [ 7060/ 7840]\n",
            "loss: 1.968933  [ 7110/ 7840]\n",
            "loss: 1.993742  [ 7160/ 7840]\n",
            "loss: 1.905557  [ 7210/ 7840]\n",
            "loss: 2.008957  [ 7260/ 7840]\n",
            "loss: 2.020051  [ 7310/ 7840]\n",
            "loss: 1.952803  [ 7360/ 7840]\n",
            "loss: 2.017754  [ 7410/ 7840]\n",
            "loss: 1.810068  [ 7460/ 7840]\n",
            "loss: 3.227051  [ 7510/ 7840]\n",
            "loss: 2.110106  [ 7560/ 7840]\n",
            "loss: 2.025852  [ 7610/ 7840]\n",
            "loss: 2.046240  [ 7660/ 7840]\n",
            "loss: 2.132697  [ 7710/ 7840]\n",
            "loss: 2.174751  [ 7760/ 7840]\n",
            "loss: 2.024740  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 16.8%, Avg loss: 2.048260 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.021324  [   10/ 7840]\n",
            "loss: 1.975499  [   60/ 7840]\n",
            "loss: 2.189191  [  110/ 7840]\n",
            "loss: 2.164597  [  160/ 7840]\n",
            "loss: 2.008929  [  210/ 7840]\n",
            "loss: 2.071558  [  260/ 7840]\n",
            "loss: 2.026071  [  310/ 7840]\n",
            "loss: 1.774698  [  360/ 7840]\n",
            "loss: 2.161892  [  410/ 7840]\n",
            "loss: 2.219692  [  460/ 7840]\n",
            "loss: 1.988377  [  510/ 7840]\n",
            "loss: 2.101859  [  560/ 7840]\n",
            "loss: 2.004800  [  610/ 7840]\n",
            "loss: 2.213680  [  660/ 7840]\n",
            "loss: 2.035422  [  710/ 7840]\n",
            "loss: 1.717745  [  760/ 7840]\n",
            "loss: 2.115327  [  810/ 7840]\n",
            "loss: 1.933786  [  860/ 7840]\n",
            "loss: 1.956452  [  910/ 7840]\n",
            "loss: 2.057535  [  960/ 7840]\n",
            "loss: 2.048068  [ 1010/ 7840]\n",
            "loss: 1.978665  [ 1060/ 7840]\n",
            "loss: 2.158416  [ 1110/ 7840]\n",
            "loss: 2.013693  [ 1160/ 7840]\n",
            "loss: 2.124878  [ 1210/ 7840]\n",
            "loss: 2.049741  [ 1260/ 7840]\n",
            "loss: 2.152718  [ 1310/ 7840]\n",
            "loss: 2.018890  [ 1360/ 7840]\n",
            "loss: 1.777966  [ 1410/ 7840]\n",
            "loss: 2.118102  [ 1460/ 7840]\n",
            "loss: 2.040295  [ 1510/ 7840]\n",
            "loss: 1.968466  [ 1560/ 7840]\n",
            "loss: 1.994695  [ 1610/ 7840]\n",
            "loss: 1.762259  [ 1660/ 7840]\n",
            "loss: 1.891382  [ 1710/ 7840]\n",
            "loss: 2.071759  [ 1760/ 7840]\n",
            "loss: 2.040466  [ 1810/ 7840]\n",
            "loss: 2.186486  [ 1860/ 7840]\n",
            "loss: 1.988338  [ 1910/ 7840]\n",
            "loss: 2.066562  [ 1960/ 7840]\n",
            "loss: 2.256508  [ 2010/ 7840]\n",
            "loss: 2.002403  [ 2060/ 7840]\n",
            "loss: 2.103122  [ 2110/ 7840]\n",
            "loss: 1.835676  [ 2160/ 7840]\n",
            "loss: 2.297367  [ 2210/ 7840]\n",
            "loss: 1.864177  [ 2260/ 7840]\n",
            "loss: 2.144015  [ 2310/ 7840]\n",
            "loss: 1.929899  [ 2360/ 7840]\n",
            "loss: 2.060420  [ 2410/ 7840]\n",
            "loss: 2.030063  [ 2460/ 7840]\n",
            "loss: 1.873764  [ 2510/ 7840]\n",
            "loss: 1.952861  [ 2560/ 7840]\n",
            "loss: 2.118333  [ 2610/ 7840]\n",
            "loss: 2.147069  [ 2660/ 7840]\n",
            "loss: 1.934897  [ 2710/ 7840]\n",
            "loss: 2.119977  [ 2760/ 7840]\n",
            "loss: 2.020005  [ 2810/ 7840]\n",
            "loss: 2.255811  [ 2860/ 7840]\n",
            "loss: 1.822839  [ 2910/ 7840]\n",
            "loss: 2.040413  [ 2960/ 7840]\n",
            "loss: 2.095959  [ 3010/ 7840]\n",
            "loss: 2.058832  [ 3060/ 7840]\n",
            "loss: 1.989038  [ 3110/ 7840]\n",
            "loss: 2.063343  [ 3160/ 7840]\n",
            "loss: 2.074603  [ 3210/ 7840]\n",
            "loss: 2.169641  [ 3260/ 7840]\n",
            "loss: 1.949529  [ 3310/ 7840]\n",
            "loss: 2.107849  [ 3360/ 7840]\n",
            "loss: 1.952240  [ 3410/ 7840]\n",
            "loss: 2.010797  [ 3460/ 7840]\n",
            "loss: 2.215363  [ 3510/ 7840]\n",
            "loss: 2.025459  [ 3560/ 7840]\n",
            "loss: 2.110177  [ 3610/ 7840]\n",
            "loss: 1.742916  [ 3660/ 7840]\n",
            "loss: 2.291351  [ 3710/ 7840]\n",
            "loss: 1.846891  [ 3760/ 7840]\n",
            "loss: 2.212231  [ 3810/ 7840]\n",
            "loss: 2.064086  [ 3860/ 7840]\n",
            "loss: 2.068306  [ 3910/ 7840]\n",
            "loss: 1.994424  [ 3960/ 7840]\n",
            "loss: 1.748542  [ 4010/ 7840]\n",
            "loss: 2.098016  [ 4060/ 7840]\n",
            "loss: 2.051295  [ 4110/ 7840]\n",
            "loss: 2.128166  [ 4160/ 7840]\n",
            "loss: 1.951475  [ 4210/ 7840]\n",
            "loss: 2.117743  [ 4260/ 7840]\n",
            "loss: 2.051173  [ 4310/ 7840]\n",
            "loss: 2.063233  [ 4360/ 7840]\n",
            "loss: 1.967772  [ 4410/ 7840]\n",
            "loss: 1.594506  [ 4460/ 7840]\n",
            "loss: 2.006597  [ 4510/ 7840]\n",
            "loss: 2.064394  [ 4560/ 7840]\n",
            "loss: 2.065450  [ 4610/ 7840]\n",
            "loss: 1.974356  [ 4660/ 7840]\n",
            "loss: 2.119149  [ 4710/ 7840]\n",
            "loss: 1.964189  [ 4760/ 7840]\n",
            "loss: 1.670025  [ 4810/ 7840]\n",
            "loss: 1.940872  [ 4860/ 7840]\n",
            "loss: 1.973787  [ 4910/ 7840]\n",
            "loss: 2.059067  [ 4960/ 7840]\n",
            "loss: 2.033637  [ 5010/ 7840]\n",
            "loss: 2.096285  [ 5060/ 7840]\n",
            "loss: 2.069461  [ 5110/ 7840]\n",
            "loss: 1.997434  [ 5160/ 7840]\n",
            "loss: 1.851330  [ 5210/ 7840]\n",
            "loss: 2.002015  [ 5260/ 7840]\n",
            "loss: 2.001370  [ 5310/ 7840]\n",
            "loss: 1.765238  [ 5360/ 7840]\n",
            "loss: 1.827259  [ 5410/ 7840]\n",
            "loss: 1.905951  [ 5460/ 7840]\n",
            "loss: 1.902435  [ 5510/ 7840]\n",
            "loss: 1.910154  [ 5560/ 7840]\n",
            "loss: 1.891105  [ 5610/ 7840]\n",
            "loss: 2.145253  [ 5660/ 7840]\n",
            "loss: 1.743186  [ 5710/ 7840]\n",
            "loss: 2.120270  [ 5760/ 7840]\n",
            "loss: 1.611105  [ 5810/ 7840]\n",
            "loss: 1.900835  [ 5860/ 7840]\n",
            "loss: 2.202240  [ 5910/ 7840]\n",
            "loss: 2.260521  [ 5960/ 7840]\n",
            "loss: 1.934864  [ 6010/ 7840]\n",
            "loss: 2.100918  [ 6060/ 7840]\n",
            "loss: 1.851492  [ 6110/ 7840]\n",
            "loss: 2.176885  [ 6160/ 7840]\n",
            "loss: 1.986532  [ 6210/ 7840]\n",
            "loss: 1.834092  [ 6260/ 7840]\n",
            "loss: 2.054329  [ 6310/ 7840]\n",
            "loss: 2.183723  [ 6360/ 7840]\n",
            "loss: 1.861100  [ 6410/ 7840]\n",
            "loss: 2.058474  [ 6460/ 7840]\n",
            "loss: 1.989553  [ 6510/ 7840]\n",
            "loss: 2.160570  [ 6560/ 7840]\n",
            "loss: 2.088468  [ 6610/ 7840]\n",
            "loss: 2.138393  [ 6660/ 7840]\n",
            "loss: 2.025194  [ 6710/ 7840]\n",
            "loss: 2.025221  [ 6760/ 7840]\n",
            "loss: 1.880736  [ 6810/ 7840]\n",
            "loss: 1.991776  [ 6860/ 7840]\n",
            "loss: 1.983011  [ 6910/ 7840]\n",
            "loss: 2.106731  [ 6960/ 7840]\n",
            "loss: 2.107969  [ 7010/ 7840]\n",
            "loss: 2.034464  [ 7060/ 7840]\n",
            "loss: 1.800683  [ 7110/ 7840]\n",
            "loss: 1.942160  [ 7160/ 7840]\n",
            "loss: 2.003789  [ 7210/ 7840]\n",
            "loss: 2.117394  [ 7260/ 7840]\n",
            "loss: 1.870377  [ 7310/ 7840]\n",
            "loss: 2.167903  [ 7360/ 7840]\n",
            "loss: 1.804792  [ 7410/ 7840]\n",
            "loss: 1.858575  [ 7460/ 7840]\n",
            "loss: 2.117834  [ 7510/ 7840]\n",
            "loss: 1.754438  [ 7560/ 7840]\n",
            "loss: 2.086225  [ 7610/ 7840]\n",
            "loss: 1.937819  [ 7660/ 7840]\n",
            "loss: 2.145639  [ 7710/ 7840]\n",
            "loss: 2.159364  [ 7760/ 7840]\n",
            "loss: 1.915460  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 22.5%, Avg loss: 1.991412 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 1.892675  [   10/ 7840]\n",
            "loss: 2.059205  [   60/ 7840]\n",
            "loss: 1.904100  [  110/ 7840]\n",
            "loss: 2.003120  [  160/ 7840]\n",
            "loss: 1.838222  [  210/ 7840]\n",
            "loss: 2.185373  [  260/ 7840]\n",
            "loss: 1.886953  [  310/ 7840]\n",
            "loss: 2.040853  [  360/ 7840]\n",
            "loss: 2.128678  [  410/ 7840]\n",
            "loss: 1.853888  [  460/ 7840]\n",
            "loss: 2.027333  [  510/ 7840]\n",
            "loss: 1.998437  [  560/ 7840]\n",
            "loss: 2.154602  [  610/ 7840]\n",
            "loss: 2.050657  [  660/ 7840]\n",
            "loss: 1.811279  [  710/ 7840]\n",
            "loss: 1.774565  [  760/ 7840]\n",
            "loss: 2.230613  [  810/ 7840]\n",
            "loss: 1.975799  [  860/ 7840]\n",
            "loss: 2.006190  [  910/ 7840]\n",
            "loss: 1.932082  [  960/ 7840]\n",
            "loss: 1.753100  [ 1010/ 7840]\n",
            "loss: 1.841091  [ 1060/ 7840]\n",
            "loss: 2.003188  [ 1110/ 7840]\n",
            "loss: 1.632645  [ 1160/ 7840]\n",
            "loss: 1.790179  [ 1210/ 7840]\n",
            "loss: 2.323814  [ 1260/ 7840]\n",
            "loss: 2.923923  [ 1310/ 7840]\n",
            "loss: 2.010625  [ 1360/ 7840]\n",
            "loss: 1.786988  [ 1410/ 7840]\n",
            "loss: 1.843564  [ 1460/ 7840]\n",
            "loss: 1.965836  [ 1510/ 7840]\n",
            "loss: 2.075484  [ 1560/ 7840]\n",
            "loss: 1.774481  [ 1610/ 7840]\n",
            "loss: 2.068414  [ 1660/ 7840]\n",
            "loss: 2.134696  [ 1710/ 7840]\n",
            "loss: 1.729095  [ 1760/ 7840]\n",
            "loss: 1.827359  [ 1810/ 7840]\n",
            "loss: 2.902332  [ 1860/ 7840]\n",
            "loss: 1.780949  [ 1910/ 7840]\n",
            "loss: 1.988910  [ 1960/ 7840]\n",
            "loss: 1.914005  [ 2010/ 7840]\n",
            "loss: 1.870496  [ 2060/ 7840]\n",
            "loss: 1.988869  [ 2110/ 7840]\n",
            "loss: 1.755781  [ 2160/ 7840]\n",
            "loss: 2.007405  [ 2210/ 7840]\n",
            "loss: 2.082912  [ 2260/ 7840]\n",
            "loss: 1.765653  [ 2310/ 7840]\n",
            "loss: 2.232331  [ 2360/ 7840]\n",
            "loss: 1.938825  [ 2410/ 7840]\n",
            "loss: 2.166789  [ 2460/ 7840]\n",
            "loss: 1.920366  [ 2510/ 7840]\n",
            "loss: 1.914166  [ 2560/ 7840]\n",
            "loss: 2.094616  [ 2610/ 7840]\n",
            "loss: 1.957837  [ 2660/ 7840]\n",
            "loss: 1.791481  [ 2710/ 7840]\n",
            "loss: 1.975693  [ 2760/ 7840]\n",
            "loss: 2.117741  [ 2810/ 7840]\n",
            "loss: 1.940844  [ 2860/ 7840]\n",
            "loss: 2.044959  [ 2910/ 7840]\n",
            "loss: 2.293459  [ 2960/ 7840]\n",
            "loss: 1.854525  [ 3010/ 7840]\n",
            "loss: 2.140154  [ 3060/ 7840]\n",
            "loss: 1.980283  [ 3110/ 7840]\n",
            "loss: 1.756459  [ 3160/ 7840]\n",
            "loss: 2.201663  [ 3210/ 7840]\n",
            "loss: 2.023829  [ 3260/ 7840]\n",
            "loss: 1.728472  [ 3310/ 7840]\n",
            "loss: 1.922037  [ 3360/ 7840]\n",
            "loss: 2.136963  [ 3410/ 7840]\n",
            "loss: 2.015169  [ 3460/ 7840]\n",
            "loss: 2.020591  [ 3510/ 7840]\n",
            "loss: 2.122254  [ 3560/ 7840]\n",
            "loss: 2.343352  [ 3610/ 7840]\n",
            "loss: 1.761238  [ 3660/ 7840]\n",
            "loss: 1.852526  [ 3710/ 7840]\n",
            "loss: 2.145560  [ 3760/ 7840]\n",
            "loss: 1.854129  [ 3810/ 7840]\n",
            "loss: 1.982875  [ 3860/ 7840]\n",
            "loss: 2.037601  [ 3910/ 7840]\n",
            "loss: 1.891075  [ 3960/ 7840]\n",
            "loss: 1.823774  [ 4010/ 7840]\n",
            "loss: 1.707112  [ 4060/ 7840]\n",
            "loss: 1.965344  [ 4110/ 7840]\n",
            "loss: 2.067461  [ 4160/ 7840]\n",
            "loss: 1.921246  [ 4210/ 7840]\n",
            "loss: 2.066966  [ 4260/ 7840]\n",
            "loss: 2.067407  [ 4310/ 7840]\n",
            "loss: 1.906642  [ 4360/ 7840]\n",
            "loss: 1.996392  [ 4410/ 7840]\n",
            "loss: 1.753583  [ 4460/ 7840]\n",
            "loss: 1.788074  [ 4510/ 7840]\n",
            "loss: 1.839732  [ 4560/ 7840]\n",
            "loss: 2.190271  [ 4610/ 7840]\n",
            "loss: 1.873108  [ 4660/ 7840]\n",
            "loss: 2.067968  [ 4710/ 7840]\n",
            "loss: 1.950204  [ 4760/ 7840]\n",
            "loss: 1.854262  [ 4810/ 7840]\n",
            "loss: 2.133089  [ 4860/ 7840]\n",
            "loss: 2.096809  [ 4910/ 7840]\n",
            "loss: 1.748497  [ 4960/ 7840]\n",
            "loss: 1.592371  [ 5010/ 7840]\n",
            "loss: 1.845177  [ 5060/ 7840]\n",
            "loss: 2.219879  [ 5110/ 7840]\n",
            "loss: 2.088655  [ 5160/ 7840]\n",
            "loss: 2.057895  [ 5210/ 7840]\n",
            "loss: 1.899299  [ 5260/ 7840]\n",
            "loss: 2.199754  [ 5310/ 7840]\n",
            "loss: 2.097956  [ 5360/ 7840]\n",
            "loss: 1.637110  [ 5410/ 7840]\n",
            "loss: 1.874051  [ 5460/ 7840]\n",
            "loss: 1.932760  [ 5510/ 7840]\n",
            "loss: 1.631059  [ 5560/ 7840]\n",
            "loss: 2.032179  [ 5610/ 7840]\n",
            "loss: 1.821385  [ 5660/ 7840]\n",
            "loss: 2.190145  [ 5710/ 7840]\n",
            "loss: 1.950692  [ 5760/ 7840]\n",
            "loss: 2.006179  [ 5810/ 7840]\n",
            "loss: 1.881817  [ 5860/ 7840]\n",
            "loss: 2.178298  [ 5910/ 7840]\n",
            "loss: 1.976864  [ 5960/ 7840]\n",
            "loss: 1.940951  [ 6010/ 7840]\n",
            "loss: 2.456446  [ 6060/ 7840]\n",
            "loss: 1.886382  [ 6110/ 7840]\n",
            "loss: 1.804838  [ 6160/ 7840]\n",
            "loss: 1.920125  [ 6210/ 7840]\n",
            "loss: 2.041395  [ 6260/ 7840]\n",
            "loss: 1.974455  [ 6310/ 7840]\n",
            "loss: 2.123048  [ 6360/ 7840]\n",
            "loss: 1.772031  [ 6410/ 7840]\n",
            "loss: 1.710082  [ 6460/ 7840]\n",
            "loss: 1.898349  [ 6510/ 7840]\n",
            "loss: 1.963964  [ 6560/ 7840]\n",
            "loss: 1.901262  [ 6610/ 7840]\n",
            "loss: 2.051463  [ 6660/ 7840]\n",
            "loss: 2.106444  [ 6710/ 7840]\n",
            "loss: 2.045642  [ 6760/ 7840]\n",
            "loss: 2.101768  [ 6810/ 7840]\n",
            "loss: 1.973590  [ 6860/ 7840]\n",
            "loss: 1.923653  [ 6910/ 7840]\n",
            "loss: 1.794810  [ 6960/ 7840]\n",
            "loss: 2.060201  [ 7010/ 7840]\n",
            "loss: 1.719303  [ 7060/ 7840]\n",
            "loss: 1.886773  [ 7110/ 7840]\n",
            "loss: 2.017282  [ 7160/ 7840]\n",
            "loss: 2.163263  [ 7210/ 7840]\n",
            "loss: 2.111106  [ 7260/ 7840]\n",
            "loss: 1.782208  [ 7310/ 7840]\n",
            "loss: 2.044339  [ 7360/ 7840]\n",
            "loss: 1.958853  [ 7410/ 7840]\n",
            "loss: 1.833949  [ 7460/ 7840]\n",
            "loss: 1.975104  [ 7510/ 7840]\n",
            "loss: 1.981136  [ 7560/ 7840]\n",
            "loss: 1.750950  [ 7610/ 7840]\n",
            "loss: 2.092713  [ 7660/ 7840]\n",
            "loss: 1.994033  [ 7710/ 7840]\n",
            "loss: 1.979438  [ 7760/ 7840]\n",
            "loss: 1.867859  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 20.4%, Avg loss: 2.001448 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 1.877543  [   10/ 7840]\n",
            "loss: 2.160632  [   60/ 7840]\n",
            "loss: 2.017679  [  110/ 7840]\n",
            "loss: 1.899137  [  160/ 7840]\n",
            "loss: 1.977710  [  210/ 7840]\n",
            "loss: 2.103965  [  260/ 7840]\n",
            "loss: 2.075363  [  310/ 7840]\n",
            "loss: 2.131183  [  360/ 7840]\n",
            "loss: 1.941989  [  410/ 7840]\n",
            "loss: 2.024370  [  460/ 7840]\n",
            "loss: 1.987612  [  510/ 7840]\n",
            "loss: 2.032466  [  560/ 7840]\n",
            "loss: 1.900742  [  610/ 7840]\n",
            "loss: 1.826323  [  660/ 7840]\n",
            "loss: 1.754609  [  710/ 7840]\n",
            "loss: 2.174765  [  760/ 7840]\n",
            "loss: 1.913962  [  810/ 7840]\n",
            "loss: 2.095158  [  860/ 7840]\n",
            "loss: 1.967993  [  910/ 7840]\n",
            "loss: 2.015818  [  960/ 7840]\n",
            "loss: 2.170351  [ 1010/ 7840]\n",
            "loss: 1.905221  [ 1060/ 7840]\n",
            "loss: 2.129354  [ 1110/ 7840]\n",
            "loss: 2.637828  [ 1160/ 7840]\n",
            "loss: 1.883543  [ 1210/ 7840]\n",
            "loss: 2.015152  [ 1260/ 7840]\n",
            "loss: 1.785590  [ 1310/ 7840]\n",
            "loss: 2.121730  [ 1360/ 7840]\n",
            "loss: 2.176591  [ 1410/ 7840]\n",
            "loss: 1.965908  [ 1460/ 7840]\n",
            "loss: 1.909172  [ 1510/ 7840]\n",
            "loss: 2.196506  [ 1560/ 7840]\n",
            "loss: 1.804267  [ 1610/ 7840]\n",
            "loss: 2.008611  [ 1660/ 7840]\n",
            "loss: 2.078990  [ 1710/ 7840]\n",
            "loss: 2.200943  [ 1760/ 7840]\n",
            "loss: 1.836403  [ 1810/ 7840]\n",
            "loss: 2.238353  [ 1860/ 7840]\n",
            "loss: 1.998918  [ 1910/ 7840]\n",
            "loss: 2.046911  [ 1960/ 7840]\n",
            "loss: 1.892079  [ 2010/ 7840]\n",
            "loss: 1.668533  [ 2060/ 7840]\n",
            "loss: 2.087754  [ 2110/ 7840]\n",
            "loss: 1.969062  [ 2160/ 7840]\n",
            "loss: 1.728597  [ 2210/ 7840]\n",
            "loss: 2.420629  [ 2260/ 7840]\n",
            "loss: 1.929495  [ 2310/ 7840]\n",
            "loss: 1.954967  [ 2360/ 7840]\n",
            "loss: 1.642046  [ 2410/ 7840]\n",
            "loss: 2.045411  [ 2460/ 7840]\n",
            "loss: 2.282189  [ 2510/ 7840]\n",
            "loss: 1.888782  [ 2560/ 7840]\n",
            "loss: 1.966158  [ 2610/ 7840]\n",
            "loss: 1.917650  [ 2660/ 7840]\n",
            "loss: 1.994267  [ 2710/ 7840]\n",
            "loss: 1.855476  [ 2760/ 7840]\n",
            "loss: 2.016456  [ 2810/ 7840]\n",
            "loss: 2.325134  [ 2860/ 7840]\n",
            "loss: 1.919030  [ 2910/ 7840]\n",
            "loss: 1.911230  [ 2960/ 7840]\n",
            "loss: 2.039894  [ 3010/ 7840]\n",
            "loss: 2.106661  [ 3060/ 7840]\n",
            "loss: 2.110134  [ 3110/ 7840]\n",
            "loss: 1.826927  [ 3160/ 7840]\n",
            "loss: 2.077499  [ 3210/ 7840]\n",
            "loss: 2.017938  [ 3260/ 7840]\n",
            "loss: 2.010776  [ 3310/ 7840]\n",
            "loss: 1.907585  [ 3360/ 7840]\n",
            "loss: 1.954919  [ 3410/ 7840]\n",
            "loss: 2.096082  [ 3460/ 7840]\n",
            "loss: 1.958989  [ 3510/ 7840]\n",
            "loss: 1.839164  [ 3560/ 7840]\n",
            "loss: 1.880161  [ 3610/ 7840]\n",
            "loss: 3.201935  [ 3660/ 7840]\n",
            "loss: 1.798751  [ 3710/ 7840]\n",
            "loss: 2.053089  [ 3760/ 7840]\n",
            "loss: 1.750621  [ 3810/ 7840]\n",
            "loss: 2.054901  [ 3860/ 7840]\n",
            "loss: 2.015958  [ 3910/ 7840]\n",
            "loss: 1.909106  [ 3960/ 7840]\n",
            "loss: 2.204254  [ 4010/ 7840]\n",
            "loss: 1.873580  [ 4060/ 7840]\n",
            "loss: 1.952078  [ 4110/ 7840]\n",
            "loss: 2.109001  [ 4160/ 7840]\n",
            "loss: 1.990127  [ 4210/ 7840]\n",
            "loss: 1.989118  [ 4260/ 7840]\n",
            "loss: 1.468785  [ 4310/ 7840]\n",
            "loss: 1.497831  [ 4360/ 7840]\n",
            "loss: 1.873216  [ 4410/ 7840]\n",
            "loss: 1.923281  [ 4460/ 7840]\n",
            "loss: 1.829525  [ 4510/ 7840]\n",
            "loss: 1.584817  [ 4560/ 7840]\n",
            "loss: 2.101579  [ 4610/ 7840]\n",
            "loss: 1.958917  [ 4660/ 7840]\n",
            "loss: 1.805596  [ 4710/ 7840]\n",
            "loss: 1.964803  [ 4760/ 7840]\n",
            "loss: 1.830625  [ 4810/ 7840]\n",
            "loss: 1.883092  [ 4860/ 7840]\n",
            "loss: 2.078999  [ 4910/ 7840]\n",
            "loss: 1.989448  [ 4960/ 7840]\n",
            "loss: 1.949775  [ 5010/ 7840]\n",
            "loss: 1.843822  [ 5060/ 7840]\n",
            "loss: 2.016121  [ 5110/ 7840]\n",
            "loss: 1.877086  [ 5160/ 7840]\n",
            "loss: 2.055059  [ 5210/ 7840]\n",
            "loss: 2.280736  [ 5260/ 7840]\n",
            "loss: 1.902027  [ 5310/ 7840]\n",
            "loss: 1.974784  [ 5360/ 7840]\n",
            "loss: 2.184282  [ 5410/ 7840]\n",
            "loss: 2.053191  [ 5460/ 7840]\n",
            "loss: 2.038409  [ 5510/ 7840]\n",
            "loss: 1.975635  [ 5560/ 7840]\n",
            "loss: 2.024645  [ 5610/ 7840]\n",
            "loss: 1.990926  [ 5660/ 7840]\n",
            "loss: 2.025822  [ 5710/ 7840]\n",
            "loss: 1.995084  [ 5760/ 7840]\n",
            "loss: 1.967555  [ 5810/ 7840]\n",
            "loss: 1.863212  [ 5860/ 7840]\n",
            "loss: 2.046840  [ 5910/ 7840]\n",
            "loss: 2.076357  [ 5960/ 7840]\n",
            "loss: 2.073580  [ 6010/ 7840]\n",
            "loss: 1.621966  [ 6060/ 7840]\n",
            "loss: 2.550920  [ 6110/ 7840]\n",
            "loss: 2.355132  [ 6160/ 7840]\n",
            "loss: 1.903978  [ 6210/ 7840]\n",
            "loss: 2.302210  [ 6260/ 7840]\n",
            "loss: 1.830049  [ 6310/ 7840]\n",
            "loss: 1.561038  [ 6360/ 7840]\n",
            "loss: 2.013398  [ 6410/ 7840]\n",
            "loss: 2.179660  [ 6460/ 7840]\n",
            "loss: 2.164126  [ 6510/ 7840]\n",
            "loss: 2.027995  [ 6560/ 7840]\n",
            "loss: 1.929608  [ 6610/ 7840]\n",
            "loss: 2.137723  [ 6660/ 7840]\n",
            "loss: 2.082072  [ 6710/ 7840]\n",
            "loss: 1.856902  [ 6760/ 7840]\n",
            "loss: 2.155906  [ 6810/ 7840]\n",
            "loss: 2.088276  [ 6860/ 7840]\n",
            "loss: 2.133024  [ 6910/ 7840]\n",
            "loss: 1.943884  [ 6960/ 7840]\n",
            "loss: 1.844230  [ 7010/ 7840]\n",
            "loss: 2.129940  [ 7060/ 7840]\n",
            "loss: 2.015468  [ 7110/ 7840]\n",
            "loss: 1.910758  [ 7160/ 7840]\n",
            "loss: 2.102184  [ 7210/ 7840]\n",
            "loss: 2.197189  [ 7260/ 7840]\n",
            "loss: 1.907219  [ 7310/ 7840]\n",
            "loss: 2.055578  [ 7360/ 7840]\n",
            "loss: 2.092702  [ 7410/ 7840]\n",
            "loss: 2.038293  [ 7460/ 7840]\n",
            "loss: 2.109677  [ 7510/ 7840]\n",
            "loss: 2.072359  [ 7560/ 7840]\n",
            "loss: 2.179511  [ 7610/ 7840]\n",
            "loss: 1.921068  [ 7660/ 7840]\n",
            "loss: 1.819300  [ 7710/ 7840]\n",
            "loss: 1.980693  [ 7760/ 7840]\n",
            "loss: 1.766862  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 19.9%, Avg loss: 2.138459 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.256666  [   10/ 7840]\n",
            "loss: 1.984713  [   60/ 7840]\n",
            "loss: 2.031214  [  110/ 7840]\n",
            "loss: 2.142638  [  160/ 7840]\n",
            "loss: 1.920680  [  210/ 7840]\n",
            "loss: 2.011235  [  260/ 7840]\n",
            "loss: 2.176485  [  310/ 7840]\n",
            "loss: 2.108910  [  360/ 7840]\n",
            "loss: 2.341602  [  410/ 7840]\n",
            "loss: 2.105919  [  460/ 7840]\n",
            "loss: 1.991203  [  510/ 7840]\n",
            "loss: 2.065036  [  560/ 7840]\n",
            "loss: 1.981162  [  610/ 7840]\n",
            "loss: 1.757436  [  660/ 7840]\n",
            "loss: 1.994870  [  710/ 7840]\n",
            "loss: 2.082558  [  760/ 7840]\n",
            "loss: 2.173734  [  810/ 7840]\n",
            "loss: 2.048099  [  860/ 7840]\n",
            "loss: 2.019353  [  910/ 7840]\n",
            "loss: 2.005641  [  960/ 7840]\n",
            "loss: 1.995678  [ 1010/ 7840]\n",
            "loss: 1.992143  [ 1060/ 7840]\n",
            "loss: 1.734129  [ 1110/ 7840]\n",
            "loss: 2.148929  [ 1160/ 7840]\n",
            "loss: 2.041312  [ 1210/ 7840]\n",
            "loss: 1.844622  [ 1260/ 7840]\n",
            "loss: 1.883807  [ 1310/ 7840]\n",
            "loss: 2.093178  [ 1360/ 7840]\n",
            "loss: 1.892756  [ 1410/ 7840]\n",
            "loss: 1.791730  [ 1460/ 7840]\n",
            "loss: 1.810833  [ 1510/ 7840]\n",
            "loss: 2.034712  [ 1560/ 7840]\n",
            "loss: 2.107129  [ 1610/ 7840]\n",
            "loss: 2.018826  [ 1660/ 7840]\n",
            "loss: 1.746634  [ 1710/ 7840]\n",
            "loss: 1.977389  [ 1760/ 7840]\n",
            "loss: 1.695984  [ 1810/ 7840]\n",
            "loss: 2.209434  [ 1860/ 7840]\n",
            "loss: 2.013870  [ 1910/ 7840]\n",
            "loss: 2.120039  [ 1960/ 7840]\n",
            "loss: 1.748344  [ 2010/ 7840]\n",
            "loss: 1.964922  [ 2060/ 7840]\n",
            "loss: 1.848706  [ 2110/ 7840]\n",
            "loss: 1.959030  [ 2160/ 7840]\n",
            "loss: 2.300912  [ 2210/ 7840]\n",
            "loss: 2.301578  [ 2260/ 7840]\n",
            "loss: 1.849628  [ 2310/ 7840]\n",
            "loss: 2.001235  [ 2360/ 7840]\n",
            "loss: 1.686119  [ 2410/ 7840]\n",
            "loss: 2.225207  [ 2460/ 7840]\n",
            "loss: 1.997316  [ 2510/ 7840]\n",
            "loss: 1.816835  [ 2560/ 7840]\n",
            "loss: 2.090820  [ 2610/ 7840]\n",
            "loss: 2.131227  [ 2660/ 7840]\n",
            "loss: 2.093468  [ 2710/ 7840]\n",
            "loss: 1.876978  [ 2760/ 7840]\n",
            "loss: 1.768551  [ 2810/ 7840]\n",
            "loss: 2.041073  [ 2860/ 7840]\n",
            "loss: 2.062324  [ 2910/ 7840]\n",
            "loss: 1.980771  [ 2960/ 7840]\n",
            "loss: 1.952107  [ 3010/ 7840]\n",
            "loss: 1.993466  [ 3060/ 7840]\n",
            "loss: 2.000310  [ 3110/ 7840]\n",
            "loss: 2.109735  [ 3160/ 7840]\n",
            "loss: 1.886866  [ 3210/ 7840]\n",
            "loss: 1.951025  [ 3260/ 7840]\n",
            "loss: 1.965107  [ 3310/ 7840]\n",
            "loss: 1.995669  [ 3360/ 7840]\n",
            "loss: 2.000417  [ 3410/ 7840]\n",
            "loss: 2.275269  [ 3460/ 7840]\n",
            "loss: 1.909447  [ 3510/ 7840]\n",
            "loss: 2.071003  [ 3560/ 7840]\n",
            "loss: 1.942667  [ 3610/ 7840]\n",
            "loss: 1.993636  [ 3660/ 7840]\n",
            "loss: 2.212370  [ 3710/ 7840]\n",
            "loss: 1.955128  [ 3760/ 7840]\n",
            "loss: 1.887131  [ 3810/ 7840]\n",
            "loss: 1.935974  [ 3860/ 7840]\n",
            "loss: 2.022893  [ 3910/ 7840]\n",
            "loss: 1.889207  [ 3960/ 7840]\n",
            "loss: 1.776633  [ 4010/ 7840]\n",
            "loss: 2.053467  [ 4060/ 7840]\n",
            "loss: 2.183095  [ 4110/ 7840]\n",
            "loss: 2.026728  [ 4160/ 7840]\n",
            "loss: 2.177519  [ 4210/ 7840]\n",
            "loss: 2.223197  [ 4260/ 7840]\n",
            "loss: 2.017913  [ 4310/ 7840]\n",
            "loss: 1.912161  [ 4360/ 7840]\n",
            "loss: 1.855367  [ 4410/ 7840]\n",
            "loss: 2.191823  [ 4460/ 7840]\n",
            "loss: 2.082675  [ 4510/ 7840]\n",
            "loss: 2.037911  [ 4560/ 7840]\n",
            "loss: 2.291337  [ 4610/ 7840]\n",
            "loss: 2.043467  [ 4660/ 7840]\n",
            "loss: 2.163416  [ 4710/ 7840]\n",
            "loss: 2.063488  [ 4760/ 7840]\n",
            "loss: 2.079176  [ 4810/ 7840]\n",
            "loss: 2.023530  [ 4860/ 7840]\n",
            "loss: 1.866652  [ 4910/ 7840]\n",
            "loss: 1.910260  [ 4960/ 7840]\n",
            "loss: 1.918432  [ 5010/ 7840]\n",
            "loss: 1.717259  [ 5060/ 7840]\n",
            "loss: 2.358347  [ 5110/ 7840]\n",
            "loss: 2.111089  [ 5160/ 7840]\n",
            "loss: 2.301425  [ 5210/ 7840]\n",
            "loss: 1.917246  [ 5260/ 7840]\n",
            "loss: 2.125714  [ 5310/ 7840]\n",
            "loss: 1.991250  [ 5360/ 7840]\n",
            "loss: 1.952301  [ 5410/ 7840]\n",
            "loss: 1.770517  [ 5460/ 7840]\n",
            "loss: 2.069474  [ 5510/ 7840]\n",
            "loss: 1.854190  [ 5560/ 7840]\n",
            "loss: 1.953363  [ 5610/ 7840]\n",
            "loss: 1.941568  [ 5660/ 7840]\n",
            "loss: 2.141918  [ 5710/ 7840]\n",
            "loss: 2.092173  [ 5760/ 7840]\n",
            "loss: 1.787883  [ 5810/ 7840]\n",
            "loss: 2.168651  [ 5860/ 7840]\n",
            "loss: 2.086816  [ 5910/ 7840]\n",
            "loss: 2.039922  [ 5960/ 7840]\n",
            "loss: 2.035469  [ 6010/ 7840]\n",
            "loss: 2.041705  [ 6060/ 7840]\n",
            "loss: 2.004803  [ 6110/ 7840]\n",
            "loss: 2.039240  [ 6160/ 7840]\n",
            "loss: 1.844296  [ 6210/ 7840]\n",
            "loss: 1.984082  [ 6260/ 7840]\n",
            "loss: 1.609820  [ 6310/ 7840]\n",
            "loss: 2.105692  [ 6360/ 7840]\n",
            "loss: 1.975301  [ 6410/ 7840]\n",
            "loss: 1.918016  [ 6460/ 7840]\n",
            "loss: 1.960384  [ 6510/ 7840]\n",
            "loss: 1.772077  [ 6560/ 7840]\n",
            "loss: 2.473362  [ 6610/ 7840]\n",
            "loss: 1.859145  [ 6660/ 7840]\n",
            "loss: 1.860911  [ 6710/ 7840]\n",
            "loss: 1.881002  [ 6760/ 7840]\n",
            "loss: 1.673254  [ 6810/ 7840]\n",
            "loss: 2.063873  [ 6860/ 7840]\n",
            "loss: 1.959392  [ 6910/ 7840]\n",
            "loss: 2.078146  [ 6960/ 7840]\n",
            "loss: 1.690317  [ 7010/ 7840]\n",
            "loss: 1.922897  [ 7060/ 7840]\n",
            "loss: 2.100288  [ 7110/ 7840]\n",
            "loss: 1.834083  [ 7160/ 7840]\n",
            "loss: 1.770115  [ 7210/ 7840]\n",
            "loss: 1.965758  [ 7260/ 7840]\n",
            "loss: 1.570711  [ 7310/ 7840]\n",
            "loss: 2.054993  [ 7360/ 7840]\n",
            "loss: 1.959246  [ 7410/ 7840]\n",
            "loss: 2.412561  [ 7460/ 7840]\n",
            "loss: 1.974965  [ 7510/ 7840]\n",
            "loss: 2.098570  [ 7560/ 7840]\n",
            "loss: 1.994484  [ 7610/ 7840]\n",
            "loss: 2.118571  [ 7660/ 7840]\n",
            "loss: 1.913788  [ 7710/ 7840]\n",
            "loss: 2.003761  [ 7760/ 7840]\n",
            "loss: 2.138360  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 14.4%, Avg loss: 2.063918 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 1.953245  [   10/ 7840]\n",
            "loss: 2.010278  [   60/ 7840]\n",
            "loss: 2.016934  [  110/ 7840]\n",
            "loss: 2.068661  [  160/ 7840]\n",
            "loss: 1.917553  [  210/ 7840]\n",
            "loss: 2.051752  [  260/ 7840]\n",
            "loss: 2.097594  [  310/ 7840]\n",
            "loss: 2.097945  [  360/ 7840]\n",
            "loss: 1.971748  [  410/ 7840]\n",
            "loss: 2.064075  [  460/ 7840]\n",
            "loss: 2.101313  [  510/ 7840]\n",
            "loss: 2.020776  [  560/ 7840]\n",
            "loss: 1.943534  [  610/ 7840]\n",
            "loss: 2.076543  [  660/ 7840]\n",
            "loss: 2.069191  [  710/ 7840]\n",
            "loss: 2.031467  [  760/ 7840]\n",
            "loss: 2.092972  [  810/ 7840]\n",
            "loss: 2.010020  [  860/ 7840]\n",
            "loss: 2.094177  [  910/ 7840]\n",
            "loss: 1.942353  [  960/ 7840]\n",
            "loss: 2.238968  [ 1010/ 7840]\n",
            "loss: 2.009144  [ 1060/ 7840]\n",
            "loss: 2.114639  [ 1110/ 7840]\n",
            "loss: 2.106475  [ 1160/ 7840]\n",
            "loss: 1.938689  [ 1210/ 7840]\n",
            "loss: 2.045578  [ 1260/ 7840]\n",
            "loss: 1.949703  [ 1310/ 7840]\n",
            "loss: 1.790749  [ 1360/ 7840]\n",
            "loss: 2.124279  [ 1410/ 7840]\n",
            "loss: 1.759715  [ 1460/ 7840]\n",
            "loss: 2.093031  [ 1510/ 7840]\n",
            "loss: 1.781621  [ 1560/ 7840]\n",
            "loss: 2.056933  [ 1610/ 7840]\n",
            "loss: 1.673853  [ 1660/ 7840]\n",
            "loss: 1.972013  [ 1710/ 7840]\n",
            "loss: 1.998164  [ 1760/ 7840]\n",
            "loss: 2.028108  [ 1810/ 7840]\n",
            "loss: 1.887115  [ 1860/ 7840]\n",
            "loss: 2.030589  [ 1910/ 7840]\n",
            "loss: 1.885524  [ 1960/ 7840]\n",
            "loss: 2.065828  [ 2010/ 7840]\n",
            "loss: 2.050280  [ 2060/ 7840]\n",
            "loss: 1.690752  [ 2110/ 7840]\n",
            "loss: 1.755532  [ 2160/ 7840]\n",
            "loss: 1.869339  [ 2210/ 7840]\n",
            "loss: 1.774623  [ 2260/ 7840]\n",
            "loss: 1.903497  [ 2310/ 7840]\n",
            "loss: 2.420681  [ 2360/ 7840]\n",
            "loss: 2.003483  [ 2410/ 7840]\n",
            "loss: 2.200477  [ 2460/ 7840]\n",
            "loss: 2.139315  [ 2510/ 7840]\n",
            "loss: 2.092892  [ 2560/ 7840]\n",
            "loss: 1.468284  [ 2610/ 7840]\n",
            "loss: 2.029013  [ 2660/ 7840]\n",
            "loss: 2.050844  [ 2710/ 7840]\n",
            "loss: 2.016756  [ 2760/ 7840]\n",
            "loss: 1.978943  [ 2810/ 7840]\n",
            "loss: 1.937246  [ 2860/ 7840]\n",
            "loss: 1.879737  [ 2910/ 7840]\n",
            "loss: 1.844540  [ 2960/ 7840]\n",
            "loss: 1.986575  [ 3010/ 7840]\n",
            "loss: 1.989703  [ 3060/ 7840]\n",
            "loss: 2.115592  [ 3110/ 7840]\n",
            "loss: 1.801280  [ 3160/ 7840]\n",
            "loss: 1.856211  [ 3210/ 7840]\n",
            "loss: 1.945118  [ 3260/ 7840]\n",
            "loss: 1.863723  [ 3310/ 7840]\n",
            "loss: 2.011770  [ 3360/ 7840]\n",
            "loss: 2.066211  [ 3410/ 7840]\n",
            "loss: 1.918684  [ 3460/ 7840]\n",
            "loss: 2.100224  [ 3510/ 7840]\n",
            "loss: 1.954783  [ 3560/ 7840]\n",
            "loss: 2.001200  [ 3610/ 7840]\n",
            "loss: 2.022071  [ 3660/ 7840]\n",
            "loss: 2.406088  [ 3710/ 7840]\n",
            "loss: 1.576757  [ 3760/ 7840]\n",
            "loss: 1.740019  [ 3810/ 7840]\n",
            "loss: 2.162880  [ 3860/ 7840]\n",
            "loss: 1.921980  [ 3910/ 7840]\n",
            "loss: 2.009818  [ 3960/ 7840]\n",
            "loss: 1.950499  [ 4010/ 7840]\n",
            "loss: 1.978555  [ 4060/ 7840]\n",
            "loss: 1.860505  [ 4110/ 7840]\n",
            "loss: 1.893961  [ 4160/ 7840]\n",
            "loss: 2.205811  [ 4210/ 7840]\n",
            "loss: 2.047054  [ 4260/ 7840]\n",
            "loss: 1.892765  [ 4310/ 7840]\n",
            "loss: 2.040942  [ 4360/ 7840]\n",
            "loss: 1.913138  [ 4410/ 7840]\n",
            "loss: 2.074255  [ 4460/ 7840]\n",
            "loss: 2.056285  [ 4510/ 7840]\n",
            "loss: 2.067497  [ 4560/ 7840]\n",
            "loss: 2.064937  [ 4610/ 7840]\n",
            "loss: 2.036049  [ 4660/ 7840]\n",
            "loss: 2.101287  [ 4710/ 7840]\n",
            "loss: 2.085467  [ 4760/ 7840]\n",
            "loss: 2.049645  [ 4810/ 7840]\n",
            "loss: 1.979020  [ 4860/ 7840]\n",
            "loss: 1.717264  [ 4910/ 7840]\n",
            "loss: 1.944491  [ 4960/ 7840]\n",
            "loss: 2.050458  [ 5010/ 7840]\n",
            "loss: 1.838235  [ 5060/ 7840]\n",
            "loss: 2.275190  [ 5110/ 7840]\n",
            "loss: 1.949832  [ 5160/ 7840]\n",
            "loss: 1.970363  [ 5210/ 7840]\n",
            "loss: 2.034620  [ 5260/ 7840]\n",
            "loss: 2.095561  [ 5310/ 7840]\n",
            "loss: 2.045039  [ 5360/ 7840]\n",
            "loss: 2.055476  [ 5410/ 7840]\n",
            "loss: 2.206293  [ 5460/ 7840]\n",
            "loss: 1.663514  [ 5510/ 7840]\n",
            "loss: 2.086054  [ 5560/ 7840]\n",
            "loss: 2.065478  [ 5610/ 7840]\n",
            "loss: 2.156218  [ 5660/ 7840]\n",
            "loss: 2.027386  [ 5710/ 7840]\n",
            "loss: 1.953074  [ 5760/ 7840]\n",
            "loss: 2.057977  [ 5810/ 7840]\n",
            "loss: 2.111459  [ 5860/ 7840]\n",
            "loss: 1.881603  [ 5910/ 7840]\n",
            "loss: 2.068222  [ 5960/ 7840]\n",
            "loss: 1.836305  [ 6010/ 7840]\n",
            "loss: 1.860359  [ 6060/ 7840]\n",
            "loss: 2.061658  [ 6110/ 7840]\n",
            "loss: 2.023254  [ 6160/ 7840]\n",
            "loss: 2.069721  [ 6210/ 7840]\n",
            "loss: 2.080007  [ 6260/ 7840]\n",
            "loss: 2.080099  [ 6310/ 7840]\n",
            "loss: 2.025655  [ 6360/ 7840]\n",
            "loss: 2.007127  [ 6410/ 7840]\n",
            "loss: 1.869544  [ 6460/ 7840]\n",
            "loss: 1.981049  [ 6510/ 7840]\n",
            "loss: 2.018018  [ 6560/ 7840]\n",
            "loss: 2.082224  [ 6610/ 7840]\n",
            "loss: 1.849699  [ 6660/ 7840]\n",
            "loss: 2.205832  [ 6710/ 7840]\n",
            "loss: 2.029464  [ 6760/ 7840]\n",
            "loss: 2.009125  [ 6810/ 7840]\n",
            "loss: 1.986485  [ 6860/ 7840]\n",
            "loss: 2.077261  [ 6910/ 7840]\n",
            "loss: 2.126595  [ 6960/ 7840]\n",
            "loss: 1.732706  [ 7010/ 7840]\n",
            "loss: 2.164386  [ 7060/ 7840]\n",
            "loss: 1.861790  [ 7110/ 7840]\n",
            "loss: 2.024817  [ 7160/ 7840]\n",
            "loss: 2.057604  [ 7210/ 7840]\n",
            "loss: 2.041076  [ 7260/ 7840]\n",
            "loss: 2.037720  [ 7310/ 7840]\n",
            "loss: 2.052778  [ 7360/ 7840]\n",
            "loss: 2.033721  [ 7410/ 7840]\n",
            "loss: 1.957567  [ 7460/ 7840]\n",
            "loss: 1.829367  [ 7510/ 7840]\n",
            "loss: 2.118254  [ 7560/ 7840]\n",
            "loss: 2.007073  [ 7610/ 7840]\n",
            "loss: 2.081716  [ 7660/ 7840]\n",
            "loss: 1.843427  [ 7710/ 7840]\n",
            "loss: 2.128567  [ 7760/ 7840]\n",
            "loss: 2.049917  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 20.7%, Avg loss: 1.997448 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.590860  [   10/ 7840]\n",
            "loss: 1.832214  [   60/ 7840]\n",
            "loss: 2.030974  [  110/ 7840]\n",
            "loss: 1.938821  [  160/ 7840]\n",
            "loss: 2.036631  [  210/ 7840]\n",
            "loss: 1.984377  [  260/ 7840]\n",
            "loss: 2.111734  [  310/ 7840]\n",
            "loss: 1.963628  [  360/ 7840]\n",
            "loss: 1.993255  [  410/ 7840]\n",
            "loss: 2.006002  [  460/ 7840]\n",
            "loss: 1.844976  [  510/ 7840]\n",
            "loss: 1.928056  [  560/ 7840]\n",
            "loss: 1.873529  [  610/ 7840]\n",
            "loss: 2.058412  [  660/ 7840]\n",
            "loss: 1.999177  [  710/ 7840]\n",
            "loss: 2.095586  [  760/ 7840]\n",
            "loss: 2.140947  [  810/ 7840]\n",
            "loss: 2.072010  [  860/ 7840]\n",
            "loss: 2.029967  [  910/ 7840]\n",
            "loss: 2.155879  [  960/ 7840]\n",
            "loss: 1.913165  [ 1010/ 7840]\n",
            "loss: 2.059418  [ 1060/ 7840]\n",
            "loss: 1.817253  [ 1110/ 7840]\n",
            "loss: 1.822098  [ 1160/ 7840]\n",
            "loss: 2.133397  [ 1210/ 7840]\n",
            "loss: 2.009267  [ 1260/ 7840]\n",
            "loss: 2.104284  [ 1310/ 7840]\n",
            "loss: 2.053300  [ 1360/ 7840]\n",
            "loss: 2.034292  [ 1410/ 7840]\n",
            "loss: 1.864804  [ 1460/ 7840]\n",
            "loss: 1.892259  [ 1510/ 7840]\n",
            "loss: 2.039621  [ 1560/ 7840]\n",
            "loss: 2.046957  [ 1610/ 7840]\n",
            "loss: 2.061061  [ 1660/ 7840]\n",
            "loss: 1.983270  [ 1710/ 7840]\n",
            "loss: 2.040834  [ 1760/ 7840]\n",
            "loss: 2.021404  [ 1810/ 7840]\n",
            "loss: 2.246805  [ 1860/ 7840]\n",
            "loss: 2.002955  [ 1910/ 7840]\n",
            "loss: 1.996088  [ 1960/ 7840]\n",
            "loss: 1.859336  [ 2010/ 7840]\n",
            "loss: 2.016571  [ 2060/ 7840]\n",
            "loss: 2.005606  [ 2110/ 7840]\n",
            "loss: 2.130028  [ 2160/ 7840]\n",
            "loss: 1.841830  [ 2210/ 7840]\n",
            "loss: 1.957029  [ 2260/ 7840]\n",
            "loss: 2.156715  [ 2310/ 7840]\n",
            "loss: 2.066345  [ 2360/ 7840]\n",
            "loss: 2.040686  [ 2410/ 7840]\n",
            "loss: 1.954548  [ 2460/ 7840]\n",
            "loss: 1.910675  [ 2510/ 7840]\n",
            "loss: 2.115643  [ 2560/ 7840]\n",
            "loss: 2.082166  [ 2610/ 7840]\n",
            "loss: 2.059582  [ 2660/ 7840]\n",
            "loss: 2.045649  [ 2710/ 7840]\n",
            "loss: 1.929710  [ 2760/ 7840]\n",
            "loss: 2.109139  [ 2810/ 7840]\n",
            "loss: 2.109883  [ 2860/ 7840]\n",
            "loss: 1.965487  [ 2910/ 7840]\n",
            "loss: 2.036847  [ 2960/ 7840]\n",
            "loss: 2.091784  [ 3010/ 7840]\n",
            "loss: 2.066884  [ 3060/ 7840]\n",
            "loss: 2.139149  [ 3110/ 7840]\n",
            "loss: 2.045299  [ 3160/ 7840]\n",
            "loss: 2.053262  [ 3210/ 7840]\n",
            "loss: 2.020780  [ 3260/ 7840]\n",
            "loss: 1.935372  [ 3310/ 7840]\n",
            "loss: 1.668581  [ 3360/ 7840]\n",
            "loss: 2.027704  [ 3410/ 7840]\n",
            "loss: 2.122926  [ 3460/ 7840]\n",
            "loss: 2.086662  [ 3510/ 7840]\n",
            "loss: 2.050323  [ 3560/ 7840]\n",
            "loss: 1.839843  [ 3610/ 7840]\n",
            "loss: 1.962577  [ 3660/ 7840]\n",
            "loss: 2.218040  [ 3710/ 7840]\n",
            "loss: 2.090389  [ 3760/ 7840]\n",
            "loss: 1.888449  [ 3810/ 7840]\n",
            "loss: 2.097759  [ 3860/ 7840]\n",
            "loss: 2.046680  [ 3910/ 7840]\n",
            "loss: 1.889226  [ 3960/ 7840]\n",
            "loss: 1.954697  [ 4010/ 7840]\n",
            "loss: 1.545756  [ 4060/ 7840]\n",
            "loss: 2.097236  [ 4110/ 7840]\n",
            "loss: 2.071185  [ 4160/ 7840]\n",
            "loss: 1.962833  [ 4210/ 7840]\n",
            "loss: 1.912934  [ 4260/ 7840]\n",
            "loss: 2.118199  [ 4310/ 7840]\n",
            "loss: 1.863160  [ 4360/ 7840]\n",
            "loss: 2.073390  [ 4410/ 7840]\n",
            "loss: 1.917036  [ 4460/ 7840]\n",
            "loss: 2.151988  [ 4510/ 7840]\n",
            "loss: 2.033298  [ 4560/ 7840]\n",
            "loss: 1.971773  [ 4610/ 7840]\n",
            "loss: 2.030218  [ 4660/ 7840]\n",
            "loss: 2.120420  [ 4710/ 7840]\n",
            "loss: 2.006512  [ 4760/ 7840]\n",
            "loss: 2.039384  [ 4810/ 7840]\n",
            "loss: 1.906650  [ 4860/ 7840]\n",
            "loss: 2.035012  [ 4910/ 7840]\n",
            "loss: 1.900143  [ 4960/ 7840]\n",
            "loss: 2.260355  [ 5010/ 7840]\n",
            "loss: 2.138493  [ 5060/ 7840]\n",
            "loss: 1.980780  [ 5110/ 7840]\n",
            "loss: 2.080432  [ 5160/ 7840]\n",
            "loss: 2.120292  [ 5210/ 7840]\n",
            "loss: 2.166592  [ 5260/ 7840]\n",
            "loss: 2.075984  [ 5310/ 7840]\n",
            "loss: 2.155125  [ 5360/ 7840]\n",
            "loss: 2.060160  [ 5410/ 7840]\n",
            "loss: 2.025206  [ 5460/ 7840]\n",
            "loss: 2.009004  [ 5510/ 7840]\n",
            "loss: 1.967257  [ 5560/ 7840]\n",
            "loss: 1.955624  [ 5610/ 7840]\n",
            "loss: 2.063706  [ 5660/ 7840]\n",
            "loss: 2.109966  [ 5710/ 7840]\n",
            "loss: 1.992499  [ 5760/ 7840]\n",
            "loss: 1.893048  [ 5810/ 7840]\n",
            "loss: 2.101907  [ 5860/ 7840]\n",
            "loss: 1.939761  [ 5910/ 7840]\n",
            "loss: 2.096537  [ 5960/ 7840]\n",
            "loss: 2.043035  [ 6010/ 7840]\n",
            "loss: 2.138699  [ 6060/ 7840]\n",
            "loss: 2.088667  [ 6110/ 7840]\n",
            "loss: 1.973967  [ 6160/ 7840]\n",
            "loss: 1.984683  [ 6210/ 7840]\n",
            "loss: 2.043452  [ 6260/ 7840]\n",
            "loss: 2.131050  [ 6310/ 7840]\n",
            "loss: 2.106899  [ 6360/ 7840]\n",
            "loss: 1.991908  [ 6410/ 7840]\n",
            "loss: 1.797819  [ 6460/ 7840]\n",
            "loss: 2.109731  [ 6510/ 7840]\n",
            "loss: 1.875690  [ 6560/ 7840]\n",
            "loss: 2.188699  [ 6610/ 7840]\n",
            "loss: 1.780076  [ 6660/ 7840]\n",
            "loss: 2.125606  [ 6710/ 7840]\n",
            "loss: 2.173175  [ 6760/ 7840]\n",
            "loss: 2.085544  [ 6810/ 7840]\n",
            "loss: 1.977231  [ 6860/ 7840]\n",
            "loss: 1.872697  [ 6910/ 7840]\n",
            "loss: 2.087263  [ 6960/ 7840]\n",
            "loss: 2.153117  [ 7010/ 7840]\n",
            "loss: 2.015740  [ 7060/ 7840]\n",
            "loss: 2.066371  [ 7110/ 7840]\n",
            "loss: 1.957881  [ 7160/ 7840]\n",
            "loss: 2.024211  [ 7210/ 7840]\n",
            "loss: 2.159331  [ 7260/ 7840]\n",
            "loss: 2.116129  [ 7310/ 7840]\n",
            "loss: 2.064108  [ 7360/ 7840]\n",
            "loss: 1.915382  [ 7410/ 7840]\n",
            "loss: 1.669834  [ 7460/ 7840]\n",
            "loss: 2.135752  [ 7510/ 7840]\n",
            "loss: 1.886684  [ 7560/ 7840]\n",
            "loss: 2.025247  [ 7610/ 7840]\n",
            "loss: 1.916075  [ 7660/ 7840]\n",
            "loss: 2.039421  [ 7710/ 7840]\n",
            "loss: 2.012034  [ 7760/ 7840]\n",
            "loss: 2.019740  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 15.1%, Avg loss: 2.053128 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 1.922685  [   10/ 7840]\n",
            "loss: 2.102707  [   60/ 7840]\n",
            "loss: 2.024755  [  110/ 7840]\n",
            "loss: 2.040614  [  160/ 7840]\n",
            "loss: 1.728882  [  210/ 7840]\n",
            "loss: 2.027846  [  260/ 7840]\n",
            "loss: 2.103097  [  310/ 7840]\n",
            "loss: 2.120129  [  360/ 7840]\n",
            "loss: 2.072138  [  410/ 7840]\n",
            "loss: 2.076053  [  460/ 7840]\n",
            "loss: 2.078795  [  510/ 7840]\n",
            "loss: 2.032717  [  560/ 7840]\n",
            "loss: 2.025408  [  610/ 7840]\n",
            "loss: 2.070172  [  660/ 7840]\n",
            "loss: 2.134909  [  710/ 7840]\n",
            "loss: 2.059786  [  760/ 7840]\n",
            "loss: 2.069702  [  810/ 7840]\n",
            "loss: 2.096968  [  860/ 7840]\n",
            "loss: 2.064411  [  910/ 7840]\n",
            "loss: 2.096213  [  960/ 7840]\n",
            "loss: 2.088058  [ 1010/ 7840]\n",
            "loss: 2.057732  [ 1060/ 7840]\n",
            "loss: 2.041142  [ 1110/ 7840]\n",
            "loss: 2.067131  [ 1160/ 7840]\n",
            "loss: 2.137730  [ 1210/ 7840]\n",
            "loss: 2.119673  [ 1260/ 7840]\n",
            "loss: 2.044692  [ 1310/ 7840]\n",
            "loss: 2.086057  [ 1360/ 7840]\n",
            "loss: 2.125987  [ 1410/ 7840]\n",
            "loss: 2.074093  [ 1460/ 7840]\n",
            "loss: 2.079129  [ 1510/ 7840]\n",
            "loss: 2.094956  [ 1560/ 7840]\n",
            "loss: 2.072491  [ 1610/ 7840]\n",
            "loss: 2.084059  [ 1660/ 7840]\n",
            "loss: 2.081882  [ 1710/ 7840]\n",
            "loss: 2.089761  [ 1760/ 7840]\n",
            "loss: 2.074002  [ 1810/ 7840]\n",
            "loss: 2.078528  [ 1860/ 7840]\n",
            "loss: 2.060757  [ 1910/ 7840]\n",
            "loss: 2.110255  [ 1960/ 7840]\n",
            "loss: 2.107572  [ 2010/ 7840]\n",
            "loss: 2.119155  [ 2060/ 7840]\n",
            "loss: 2.091710  [ 2110/ 7840]\n",
            "loss: 2.088663  [ 2160/ 7840]\n",
            "loss: 2.092808  [ 2210/ 7840]\n",
            "loss: 2.108506  [ 2260/ 7840]\n",
            "loss: 2.073844  [ 2310/ 7840]\n",
            "loss: 2.047233  [ 2360/ 7840]\n",
            "loss: 2.088171  [ 2410/ 7840]\n",
            "loss: 2.112532  [ 2460/ 7840]\n",
            "loss: 2.116018  [ 2510/ 7840]\n",
            "loss: 2.059682  [ 2560/ 7840]\n",
            "loss: 2.099874  [ 2610/ 7840]\n",
            "loss: 2.066322  [ 2660/ 7840]\n",
            "loss: 2.058401  [ 2710/ 7840]\n",
            "loss: 2.095369  [ 2760/ 7840]\n",
            "loss: 2.091370  [ 2810/ 7840]\n",
            "loss: 2.078415  [ 2860/ 7840]\n",
            "loss: 2.104365  [ 2910/ 7840]\n",
            "loss: 2.077168  [ 2960/ 7840]\n",
            "loss: 2.058318  [ 3010/ 7840]\n",
            "loss: 2.081660  [ 3060/ 7840]\n",
            "loss: 2.060097  [ 3110/ 7840]\n",
            "loss: 2.084206  [ 3160/ 7840]\n",
            "loss: 2.083998  [ 3210/ 7840]\n",
            "loss: 2.115104  [ 3260/ 7840]\n",
            "loss: 2.076218  [ 3310/ 7840]\n",
            "loss: 2.096008  [ 3360/ 7840]\n",
            "loss: 2.087744  [ 3410/ 7840]\n",
            "loss: 2.071123  [ 3460/ 7840]\n",
            "loss: 2.062590  [ 3510/ 7840]\n",
            "loss: 2.057223  [ 3560/ 7840]\n",
            "loss: 2.053287  [ 3610/ 7840]\n",
            "loss: 2.079291  [ 3660/ 7840]\n",
            "loss: 2.090328  [ 3710/ 7840]\n",
            "loss: 2.086482  [ 3760/ 7840]\n",
            "loss: 2.081029  [ 3810/ 7840]\n",
            "loss: 2.075694  [ 3860/ 7840]\n",
            "loss: 2.074769  [ 3910/ 7840]\n",
            "loss: 2.094929  [ 3960/ 7840]\n",
            "loss: 2.053999  [ 4010/ 7840]\n",
            "loss: 2.084957  [ 4060/ 7840]\n",
            "loss: 2.088181  [ 4110/ 7840]\n",
            "loss: 2.082992  [ 4160/ 7840]\n",
            "loss: 2.128804  [ 4210/ 7840]\n",
            "loss: 2.063945  [ 4260/ 7840]\n",
            "loss: 2.089903  [ 4310/ 7840]\n",
            "loss: 2.069438  [ 4360/ 7840]\n",
            "loss: 2.093636  [ 4410/ 7840]\n",
            "loss: 2.082135  [ 4460/ 7840]\n",
            "loss: 2.120641  [ 4510/ 7840]\n",
            "loss: 2.082679  [ 4560/ 7840]\n",
            "loss: 2.057251  [ 4610/ 7840]\n",
            "loss: 2.125205  [ 4660/ 7840]\n",
            "loss: 2.096658  [ 4710/ 7840]\n",
            "loss: 2.060213  [ 4760/ 7840]\n",
            "loss: 2.098607  [ 4810/ 7840]\n",
            "loss: 2.085700  [ 4860/ 7840]\n",
            "loss: 2.088486  [ 4910/ 7840]\n",
            "loss: 2.077334  [ 4960/ 7840]\n",
            "loss: 2.080986  [ 5010/ 7840]\n",
            "loss: 2.097396  [ 5060/ 7840]\n",
            "loss: 2.088962  [ 5110/ 7840]\n",
            "loss: 2.102245  [ 5160/ 7840]\n",
            "loss: 2.046961  [ 5210/ 7840]\n",
            "loss: 2.103425  [ 5260/ 7840]\n",
            "loss: 2.093441  [ 5310/ 7840]\n",
            "loss: 2.069428  [ 5360/ 7840]\n",
            "loss: 2.078445  [ 5410/ 7840]\n",
            "loss: 2.079415  [ 5460/ 7840]\n",
            "loss: 2.082397  [ 5510/ 7840]\n",
            "loss: 2.063405  [ 5560/ 7840]\n",
            "loss: 2.051777  [ 5610/ 7840]\n",
            "loss: 2.136097  [ 5660/ 7840]\n",
            "loss: 2.067482  [ 5710/ 7840]\n",
            "loss: 2.058525  [ 5760/ 7840]\n",
            "loss: 2.117751  [ 5810/ 7840]\n",
            "loss: 2.083881  [ 5860/ 7840]\n",
            "loss: 2.057032  [ 5910/ 7840]\n",
            "loss: 2.054504  [ 5960/ 7840]\n",
            "loss: 2.070370  [ 6010/ 7840]\n",
            "loss: 2.098905  [ 6060/ 7840]\n",
            "loss: 2.056401  [ 6110/ 7840]\n",
            "loss: 2.110580  [ 6160/ 7840]\n",
            "loss: 2.051771  [ 6210/ 7840]\n",
            "loss: 2.093563  [ 6260/ 7840]\n",
            "loss: 2.075968  [ 6310/ 7840]\n",
            "loss: 2.072172  [ 6360/ 7840]\n",
            "loss: 2.092101  [ 6410/ 7840]\n",
            "loss: 2.061915  [ 6460/ 7840]\n",
            "loss: 2.081292  [ 6510/ 7840]\n",
            "loss: 2.095460  [ 6560/ 7840]\n",
            "loss: 2.069988  [ 6610/ 7840]\n",
            "loss: 2.061695  [ 6660/ 7840]\n",
            "loss: 2.072863  [ 6710/ 7840]\n",
            "loss: 2.075737  [ 6760/ 7840]\n",
            "loss: 2.077758  [ 6810/ 7840]\n",
            "loss: 2.097664  [ 6860/ 7840]\n",
            "loss: 2.097279  [ 6910/ 7840]\n",
            "loss: 2.083326  [ 6960/ 7840]\n",
            "loss: 2.083303  [ 7010/ 7840]\n",
            "loss: 2.087325  [ 7060/ 7840]\n",
            "loss: 2.079502  [ 7110/ 7840]\n",
            "loss: 2.064283  [ 7160/ 7840]\n",
            "loss: 2.117395  [ 7210/ 7840]\n",
            "loss: 2.093918  [ 7260/ 7840]\n",
            "loss: 2.061261  [ 7310/ 7840]\n",
            "loss: 2.092916  [ 7360/ 7840]\n",
            "loss: 2.090312  [ 7410/ 7840]\n",
            "loss: 2.091208  [ 7460/ 7840]\n",
            "loss: 2.068640  [ 7510/ 7840]\n",
            "loss: 2.076325  [ 7560/ 7840]\n",
            "loss: 2.093096  [ 7610/ 7840]\n",
            "loss: 2.058130  [ 7660/ 7840]\n",
            "loss: 2.071097  [ 7710/ 7840]\n",
            "loss: 2.085919  [ 7760/ 7840]\n",
            "loss: 2.104813  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 13.2%, Avg loss: 2.081326 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.096181  [   10/ 7840]\n",
            "loss: 2.070134  [   60/ 7840]\n",
            "loss: 2.087113  [  110/ 7840]\n",
            "loss: 2.076344  [  160/ 7840]\n",
            "loss: 2.086116  [  210/ 7840]\n",
            "loss: 2.053895  [  260/ 7840]\n",
            "loss: 2.055390  [  310/ 7840]\n",
            "loss: 2.097224  [  360/ 7840]\n",
            "loss: 2.077512  [  410/ 7840]\n",
            "loss: 2.080575  [  460/ 7840]\n",
            "loss: 2.058187  [  510/ 7840]\n",
            "loss: 2.065370  [  560/ 7840]\n",
            "loss: 2.093450  [  610/ 7840]\n",
            "loss: 2.099645  [  660/ 7840]\n",
            "loss: 2.094082  [  710/ 7840]\n",
            "loss: 2.056329  [  760/ 7840]\n",
            "loss: 2.091490  [  810/ 7840]\n",
            "loss: 2.071697  [  860/ 7840]\n",
            "loss: 2.118130  [  910/ 7840]\n",
            "loss: 2.105510  [  960/ 7840]\n",
            "loss: 2.083742  [ 1010/ 7840]\n",
            "loss: 2.067200  [ 1060/ 7840]\n",
            "loss: 2.073959  [ 1110/ 7840]\n",
            "loss: 2.059253  [ 1160/ 7840]\n",
            "loss: 2.067189  [ 1210/ 7840]\n",
            "loss: 2.087353  [ 1260/ 7840]\n",
            "loss: 2.098642  [ 1310/ 7840]\n",
            "loss: 2.082854  [ 1360/ 7840]\n",
            "loss: 2.099365  [ 1410/ 7840]\n",
            "loss: 2.091108  [ 1460/ 7840]\n",
            "loss: 2.055013  [ 1510/ 7840]\n",
            "loss: 2.101329  [ 1560/ 7840]\n",
            "loss: 2.109470  [ 1610/ 7840]\n",
            "loss: 2.068383  [ 1660/ 7840]\n",
            "loss: 2.049129  [ 1710/ 7840]\n",
            "loss: 2.077624  [ 1760/ 7840]\n",
            "loss: 2.106092  [ 1810/ 7840]\n",
            "loss: 2.083758  [ 1860/ 7840]\n",
            "loss: 2.040139  [ 1910/ 7840]\n",
            "loss: 2.075832  [ 1960/ 7840]\n",
            "loss: 2.092238  [ 2010/ 7840]\n",
            "loss: 2.099782  [ 2060/ 7840]\n",
            "loss: 2.089264  [ 2110/ 7840]\n",
            "loss: 2.099539  [ 2160/ 7840]\n",
            "loss: 2.071088  [ 2210/ 7840]\n",
            "loss: 2.082791  [ 2260/ 7840]\n",
            "loss: 2.087951  [ 2310/ 7840]\n",
            "loss: 2.074121  [ 2360/ 7840]\n",
            "loss: 2.066391  [ 2410/ 7840]\n",
            "loss: 2.080599  [ 2460/ 7840]\n",
            "loss: 2.073728  [ 2510/ 7840]\n",
            "loss: 2.081779  [ 2560/ 7840]\n",
            "loss: 2.077018  [ 2610/ 7840]\n",
            "loss: 2.098472  [ 2660/ 7840]\n",
            "loss: 2.062569  [ 2710/ 7840]\n",
            "loss: 2.094109  [ 2760/ 7840]\n",
            "loss: 2.047344  [ 2810/ 7840]\n",
            "loss: 2.083464  [ 2860/ 7840]\n",
            "loss: 2.095695  [ 2910/ 7840]\n",
            "loss: 2.116788  [ 2960/ 7840]\n",
            "loss: 2.087088  [ 3010/ 7840]\n",
            "loss: 2.083448  [ 3060/ 7840]\n",
            "loss: 2.077216  [ 3110/ 7840]\n",
            "loss: 2.072224  [ 3160/ 7840]\n",
            "loss: 2.078445  [ 3210/ 7840]\n",
            "loss: 2.091023  [ 3260/ 7840]\n",
            "loss: 2.073667  [ 3310/ 7840]\n",
            "loss: 2.066722  [ 3360/ 7840]\n",
            "loss: 2.081192  [ 3410/ 7840]\n",
            "loss: 2.084689  [ 3460/ 7840]\n",
            "loss: 2.069567  [ 3510/ 7840]\n",
            "loss: 2.058520  [ 3560/ 7840]\n",
            "loss: 2.081607  [ 3610/ 7840]\n",
            "loss: 2.076501  [ 3660/ 7840]\n",
            "loss: 2.055753  [ 3710/ 7840]\n",
            "loss: 2.104059  [ 3760/ 7840]\n",
            "loss: 2.089434  [ 3810/ 7840]\n",
            "loss: 2.064537  [ 3860/ 7840]\n",
            "loss: 2.095622  [ 3910/ 7840]\n",
            "loss: 2.094784  [ 3960/ 7840]\n",
            "loss: 2.071478  [ 4010/ 7840]\n",
            "loss: 2.079663  [ 4060/ 7840]\n",
            "loss: 2.078662  [ 4110/ 7840]\n",
            "loss: 2.097123  [ 4160/ 7840]\n",
            "loss: 2.071028  [ 4210/ 7840]\n",
            "loss: 2.084874  [ 4260/ 7840]\n",
            "loss: 2.093446  [ 4310/ 7840]\n",
            "loss: 2.074284  [ 4360/ 7840]\n",
            "loss: 2.089535  [ 4410/ 7840]\n",
            "loss: 2.063172  [ 4460/ 7840]\n",
            "loss: 2.084211  [ 4510/ 7840]\n",
            "loss: 2.110753  [ 4560/ 7840]\n",
            "loss: 2.070669  [ 4610/ 7840]\n",
            "loss: 2.076472  [ 4660/ 7840]\n",
            "loss: 2.045600  [ 4710/ 7840]\n",
            "loss: 2.134007  [ 4760/ 7840]\n",
            "loss: 2.093619  [ 4810/ 7840]\n",
            "loss: 2.091072  [ 4860/ 7840]\n",
            "loss: 2.078869  [ 4910/ 7840]\n",
            "loss: 2.036237  [ 4960/ 7840]\n",
            "loss: 2.038608  [ 5010/ 7840]\n",
            "loss: 2.068748  [ 5060/ 7840]\n",
            "loss: 2.077554  [ 5110/ 7840]\n",
            "loss: 2.069911  [ 5160/ 7840]\n",
            "loss: 2.088515  [ 5210/ 7840]\n",
            "loss: 2.065446  [ 5260/ 7840]\n",
            "loss: 2.106789  [ 5310/ 7840]\n",
            "loss: 2.059797  [ 5360/ 7840]\n",
            "loss: 2.113146  [ 5410/ 7840]\n",
            "loss: 2.082513  [ 5460/ 7840]\n",
            "loss: 2.029906  [ 5510/ 7840]\n",
            "loss: 2.082443  [ 5560/ 7840]\n",
            "loss: 2.027404  [ 5610/ 7840]\n",
            "loss: 2.070888  [ 5660/ 7840]\n",
            "loss: 2.087105  [ 5710/ 7840]\n",
            "loss: 2.086477  [ 5760/ 7840]\n",
            "loss: 2.072489  [ 5810/ 7840]\n",
            "loss: 2.104066  [ 5860/ 7840]\n",
            "loss: 2.126221  [ 5910/ 7840]\n",
            "loss: 2.060495  [ 5960/ 7840]\n",
            "loss: 2.150403  [ 6010/ 7840]\n",
            "loss: 2.082486  [ 6060/ 7840]\n",
            "loss: 2.089074  [ 6110/ 7840]\n",
            "loss: 2.127348  [ 6160/ 7840]\n",
            "loss: 2.085748  [ 6210/ 7840]\n",
            "loss: 2.088382  [ 6260/ 7840]\n",
            "loss: 2.123611  [ 6310/ 7840]\n",
            "loss: 2.050315  [ 6360/ 7840]\n",
            "loss: 2.083003  [ 6410/ 7840]\n",
            "loss: 2.064805  [ 6460/ 7840]\n",
            "loss: 2.095592  [ 6510/ 7840]\n",
            "loss: 2.118577  [ 6560/ 7840]\n",
            "loss: 2.089538  [ 6610/ 7840]\n",
            "loss: 2.072478  [ 6660/ 7840]\n",
            "loss: 2.062423  [ 6710/ 7840]\n",
            "loss: 2.092876  [ 6760/ 7840]\n",
            "loss: 2.071673  [ 6810/ 7840]\n",
            "loss: 2.105649  [ 6860/ 7840]\n",
            "loss: 2.081120  [ 6910/ 7840]\n",
            "loss: 2.097861  [ 6960/ 7840]\n",
            "loss: 2.087874  [ 7010/ 7840]\n",
            "loss: 2.074387  [ 7060/ 7840]\n",
            "loss: 2.103087  [ 7110/ 7840]\n",
            "loss: 2.096592  [ 7160/ 7840]\n",
            "loss: 2.057572  [ 7210/ 7840]\n",
            "loss: 2.099332  [ 7260/ 7840]\n",
            "loss: 2.051544  [ 7310/ 7840]\n",
            "loss: 2.050618  [ 7360/ 7840]\n",
            "loss: 2.085228  [ 7410/ 7840]\n",
            "loss: 2.054576  [ 7460/ 7840]\n",
            "loss: 2.104367  [ 7510/ 7840]\n",
            "loss: 2.073738  [ 7560/ 7840]\n",
            "loss: 2.080495  [ 7610/ 7840]\n",
            "loss: 2.070392  [ 7660/ 7840]\n",
            "loss: 2.071658  [ 7710/ 7840]\n",
            "loss: 2.083247  [ 7760/ 7840]\n",
            "loss: 2.079113  [ 7810/ 7840]\n",
            "Test Error: \n",
            " Accuracy: 12.1%, Avg loss: 2.081451 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "The model will be running on cuda:0 device\n",
            "loss: 2.046237  [   10/ 7840]\n",
            "loss: 2.066910  [   60/ 7840]\n",
            "loss: 2.123111  [  110/ 7840]\n",
            "loss: 2.089728  [  160/ 7840]\n",
            "loss: 2.077640  [  210/ 7840]\n",
            "loss: 2.089410  [  260/ 7840]\n",
            "loss: 2.046530  [  310/ 7840]\n",
            "loss: 2.075876  [  360/ 7840]\n",
            "loss: 2.085387  [  410/ 7840]\n",
            "loss: 2.107908  [  460/ 7840]\n",
            "loss: 2.140432  [  510/ 7840]\n",
            "loss: 2.090980  [  560/ 7840]\n",
            "loss: 2.061640  [  610/ 7840]\n",
            "loss: 2.126826  [  660/ 7840]\n",
            "loss: 2.090897  [  710/ 7840]\n",
            "loss: 2.043916  [  760/ 7840]\n",
            "loss: 2.067346  [  810/ 7840]\n",
            "loss: 2.105162  [  860/ 7840]\n",
            "loss: 2.050785  [  910/ 7840]\n",
            "loss: 2.029808  [  960/ 7840]\n",
            "loss: 2.093389  [ 1010/ 7840]\n",
            "loss: 2.087848  [ 1060/ 7840]\n",
            "loss: 2.108587  [ 1110/ 7840]\n",
            "loss: 2.087125  [ 1160/ 7840]\n",
            "loss: 2.095422  [ 1210/ 7840]\n",
            "loss: 2.069109  [ 1260/ 7840]\n",
            "loss: 2.093397  [ 1310/ 7840]\n",
            "loss: 2.081747  [ 1360/ 7840]\n",
            "loss: 2.107970  [ 1410/ 7840]\n",
            "loss: 2.075990  [ 1460/ 7840]\n",
            "loss: 2.066102  [ 1510/ 7840]\n",
            "loss: 2.030873  [ 1560/ 7840]\n",
            "loss: 2.098315  [ 1610/ 7840]\n",
            "loss: 2.073426  [ 1660/ 7840]\n",
            "loss: 2.083413  [ 1710/ 7840]\n",
            "loss: 2.117964  [ 1760/ 7840]\n",
            "loss: 2.075257  [ 1810/ 7840]\n",
            "loss: 2.045488  [ 1860/ 7840]\n",
            "loss: 2.071034  [ 1910/ 7840]\n",
            "loss: 2.111470  [ 1960/ 7840]\n",
            "loss: 2.081908  [ 2010/ 7840]\n",
            "loss: 2.093618  [ 2060/ 7840]\n",
            "loss: 2.044062  [ 2110/ 7840]\n",
            "loss: 2.045635  [ 2160/ 7840]\n",
            "loss: 2.074704  [ 2210/ 7840]\n",
            "loss: 2.097651  [ 2260/ 7840]\n",
            "loss: 2.096123  [ 2310/ 7840]\n",
            "loss: 2.094630  [ 2360/ 7840]\n",
            "loss: 2.084336  [ 2410/ 7840]\n",
            "loss: 2.077352  [ 2460/ 7840]\n",
            "loss: 2.070160  [ 2510/ 7840]\n",
            "loss: 2.082551  [ 2560/ 7840]\n",
            "loss: 2.087534  [ 2610/ 7840]\n",
            "loss: 2.083732  [ 2660/ 7840]\n",
            "loss: 2.075689  [ 2710/ 7840]\n",
            "loss: 2.055334  [ 2760/ 7840]\n",
            "loss: 2.111308  [ 2810/ 7840]\n",
            "loss: 2.076896  [ 2860/ 7840]\n",
            "loss: 2.097803  [ 2910/ 7840]\n",
            "loss: 2.075863  [ 2960/ 7840]\n",
            "loss: 2.119327  [ 3010/ 7840]\n",
            "loss: 2.056568  [ 3060/ 7840]\n",
            "loss: 2.075202  [ 3110/ 7840]\n",
            "loss: 2.053081  [ 3160/ 7840]\n",
            "loss: 2.067033  [ 3210/ 7840]\n",
            "loss: 2.101119  [ 3260/ 7840]\n",
            "loss: 2.090152  [ 3310/ 7840]\n",
            "loss: 2.089208  [ 3360/ 7840]\n",
            "loss: 2.104408  [ 3410/ 7840]\n",
            "loss: 2.100196  [ 3460/ 7840]\n",
            "loss: 2.075692  [ 3510/ 7840]\n",
            "loss: 2.066828  [ 3560/ 7840]\n",
            "loss: 2.069377  [ 3610/ 7840]\n",
            "loss: 2.091005  [ 3660/ 7840]\n",
            "loss: 2.108152  [ 3710/ 7840]\n",
            "loss: 2.084367  [ 3760/ 7840]\n",
            "loss: 2.067336  [ 3810/ 7840]\n",
            "loss: 2.071701  [ 3860/ 7840]\n",
            "loss: 2.083698  [ 3910/ 7840]\n",
            "loss: 2.085338  [ 3960/ 7840]\n",
            "loss: 2.102744  [ 4010/ 7840]\n",
            "loss: 2.084520  [ 4060/ 7840]\n",
            "loss: 2.078721  [ 4110/ 7840]\n",
            "loss: 2.086851  [ 4160/ 7840]\n",
            "loss: 2.083404  [ 4210/ 7840]\n",
            "loss: 2.059723  [ 4260/ 7840]\n",
            "loss: 2.075175  [ 4310/ 7840]\n",
            "loss: 2.124913  [ 4360/ 7840]\n",
            "loss: 2.090449  [ 4410/ 7840]\n",
            "loss: 2.106546  [ 4460/ 7840]\n",
            "loss: 2.084808  [ 4510/ 7840]\n",
            "loss: 2.082196  [ 4560/ 7840]\n",
            "loss: 2.121294  [ 4610/ 7840]\n",
            "loss: 2.065516  [ 4660/ 7840]\n",
            "loss: 2.084193  [ 4710/ 7840]\n",
            "loss: 2.052702  [ 4760/ 7840]\n",
            "loss: 2.073958  [ 4810/ 7840]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14124\\1140447656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14124\\2810639803.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# get the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#X = Variable(torch.tensor(images))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14124\\1775717924.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    961\u001b[0m         \"\"\"\n\u001b[0;32m    962\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresized_crop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresized_crop\u001b[1;34m(img, top, left, height, width, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresized_crop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\mario\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2080\u001b[0m                 )\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2084\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "learning_rate = 0.0001 \n",
        "# Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "    if t+1 % 10 == 0:\n",
        "      torch.save(model.state_dict(), f\"./models/model_{time.monotonic_ns()}.pth\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nSonA8SYR_CA",
      "metadata": {
        "id": "nSonA8SYR_CA"
      },
      "outputs": [],
      "source": [
        "#@title Give me a name {display-mode: \"form\"}\n",
        "\n",
        "# This code will be hidden when the notebook is loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73439e46-b756-4012-bd09-ae344f14f63d",
      "metadata": {
        "id": "73439e46-b756-4012-bd09-ae344f14f63d"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # PyTorch thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e9645c-1d20-4654-8dc4-10e2eb84f2d6",
      "metadata": {
        "id": "85e9645c-1d20-4654-8dc4-10e2eb84f2d6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect() # Python thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde0c918-3686-4085-9f2b-5caba200cb88",
      "metadata": {
        "id": "bde0c918-3686-4085-9f2b-5caba200cb88"
      },
      "outputs": [],
      "source": [
        "model = Network()\n",
        "model.load_state_dict(torch.load(f\"./models/model_3456965530940.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bd300f1-f7cc-40f3-a70f-f2b86b0df532",
      "metadata": {
        "id": "5bd300f1-f7cc-40f3-a70f-f2b86b0df532"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for inputs, labels in test_dataloader:\n",
        "        output = model(inputs) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "        \n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Lovy0qCfDTj",
      "metadata": {
        "id": "_Lovy0qCfDTj"
      },
      "outputs": [],
      "source": [
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
